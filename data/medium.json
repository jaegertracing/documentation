{
  "+p_xml": "version=\"1.0\" encoding=\"UTF-8\"",
  "rss": {
    "+@xmlns:dc": "http://purl.org/dc/elements/1.1/",
    "+@xmlns:content": "http://purl.org/rss/1.0/modules/content/",
    "+@xmlns:atom": "http://www.w3.org/2005/Atom",
    "+@version": "2.0",
    "+@xmlns:cc": "http://cyber.law.harvard.edu/rss/creativeCommonsRssModule.html",
    "channel": {
      "title": "JaegerTracing - Medium",
      "description": "Open source distributed tracing platform at Cloud Native Computing Foundation (graduated). https://jaegertracing.io - Medium",
      "link": [
        "https://medium.com/jaegertracing?source=rss----99735986d50---4",
        {
          "+@href": "https://medium.com/feed/jaegertracing",
          "+@rel": "self",
          "+@type": "application/rss+xml"
        },
        {
          "+@href": "http://medium.superfeedr.com",
          "+@rel": "hub"
        }
      ],
      "image": {
        "url": "https://cdn-images-1.medium.com/proxy/1*TGH72Nnw24QL3iV9IOm4VA.png",
        "title": "JaegerTracing - Medium",
        "link": "https://medium.com/jaegertracing?source=rss----99735986d50---4"
      },
      "generator": "Medium",
      "lastBuildDate": "Sat, 17 May 2025 04:52:00 GMT",
      "webMaster": "yourfriends@medium.com",
      "item": [
        {
          "title": "Jaeger v2 released",
          "link": "https://medium.com/jaegertracing/jaeger-v2-released-09a6033d1b10?source=rss----99735986d50---4",
          "guid": {
            "+content": "https://medium.com/p/09a6033d1b10",
            "+@isPermaLink": "false"
          },
          "category": [
            "major-release",
            "distributed-tracing",
            "jaeger",
            "v2",
            "opentelemetry"
          ],
          "creator": "Yuri Shkuro",
          "pubDate": "Sat, 23 Nov 2024 18:33:07 GMT",
          "updated": "2025-01-02T23:56:04.707Z",
          "encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ccqn-B5CfJhwNkjFjqAFLg.jpeg\" /></figure><p><a href=\"https://www.jaegertracing.io/\"><strong>Jaeger</strong></a>, the popular open-source distributed tracing platform, has had a successful 9 year history as being one of the first graduated projects in the Cloud Native Computing Foundation (CNCF). After over 60 releases, Jaeger is celebrating a major milestone with the release of Jaeger v2. This new version is a new architecture for Jaeger components that utilizes <a href=\"https://opentelemetry.io/docs/collector/\"><strong>OpenTelemetry Collector</strong></a> framework as the base and extends it to implement Jaegerâ€™s unique features. It brings significant improvements and changes, making Jaeger more flexible, extensible, and better aligned with the OpenTelemetry project.</p><p>In this blog post, weâ€™ll dive into the details of Jaeger v2, exploring its design, features, and benefits. Sharing what users can expect from this exciting new release and what is next for theÂ project.</p><h3>OpenTelemetry Foundation <strong>ğŸ§±</strong></h3><p>OpenTelemetry is the de-facto standard for application instrumentation providing the foundation for observability. Jaeger is now based on the cornerstone of this project, the OpenTelemetry Collector. Jaeger is a complete tracing platform that includes storage and the UI. OpenTelemetry Collector is usually an intermediate component in the collection pipelines that are used to receive, process, transform, and export different telemetry types. The two systems have some overlaps, for example, `jaeger-agent` and `jaeger-collector` play a role similar to what can be done with OpenTelemetry Collector, but only forÂ traces.</p><p>Historically, both Jaeger and OpenTelemetry Collector reused each otherâ€™s code. Collector supports receivers for legacy Jaeger formats implemented by importing Jaeger packages. And Jaeger reuses Collectorâ€™s receivers and data model converters. Because of this synergy, itâ€™s been our goal for a while to bring the two projectsÂ closer.</p><p>OpenTelemetry Collector has a very flexible and extensible design, which makes it easy to extend with additional components needed for Jaeger useÂ cases.</p><h3>Features and BenefitsÂ ğŸ’¡</h3><p>By aligning the Jaeger v2 architecture with OpenTelemetry Collector, we deliver several exciting features and benefits for users, including:</p><ul><li><strong>Native OpenTelemetry processing</strong>: Jaeger v2 natively supports OTLP data format, eliminating the translation step from OTLP to Jaegerâ€™s internal data format and improving performance.</li><li><strong>Batched data processing</strong>: the OpenTelemetry Collector pipelines operate on batches of data, which can be especially important for storage backends like ClickHouse that are much more performant with batch inserts. Jaeger v2 will be able to utilize this batch-based internal pipelineÂ design.</li><li><strong>Familiar developer experience</strong>: Jaeger v2 follows the same configuration and deployment model as OpenTelemetry Collector, providing a more consistent developer experience.</li><li><strong>Access to OpenTelemetry Collector features</strong>: Jaeger v2 inherits all the core features of the Collector, including auth, cert reloading, internal monitoring, health checks, z-pages,Â etc.</li><li><strong>Advanced sampling capabilities: </strong>Jaeger v2 introduces the ability to perform tail-based sampling, using the upstream<a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/processor/tailsamplingprocessor\"><strong> OpenTelemetry-contrib processor</strong></a>. It also fully supports Jaegerâ€™s original<a href=\"https://www.jaegertracing.io/docs/1.62/sampling/#remote-sampling\"><strong> Remote</strong></a> and<a href=\"https://www.jaegertracing.io/docs/1.62/sampling/#adaptive-sampling\"><strong> Adaptive</strong></a> head-based samplingÂ methods.</li><li><strong>Access to OpenTelemetry Collector ecosystem</strong>: Jaeger v2 can use a multitude of extensions available for OpenTelemetry Collector, such as span-to-metric connector, telemetry rewriting processors, PII filtering, and many others. For example, we are able to reuse Kafka exporter and receiver and replicate the Jaeger v1 collector-ingester deployment model without maintaining any extraÂ code.</li></ul><p>The result is a leaner code base and more importantly the ability to future proof Jaeger as OpenTelemetry evolves to ensure Jaeger is always the first tracing system for open source users. Compatibility between OpenTelemetry and Jaeger will be supported on day 1 due to the tight integration between the projects. This will continue collaboration between both projects, getting more users to adopt open source technologies moreÂ quickly.</p><h3>Design and Architecture ğŸ›</h3><p>Overall, Jaeger v2 architecture is very similar to a standard OpenTelemetry Collector that has <em>pipelines</em> for receiving and processing telemetry (a pipeline encapsulates <em>receivers</em>, <em>processors</em>, and <em>exporters</em>), and <em>extensions</em> that perform functions not directly related to processing of telemetry. Jaeger v2 makes a few design decisions in how to use the Collector framework.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*erzuvdTb31Ff3oFI\" /></figure><h3>Query Extension ğŸ”</h3><p>Since Collector is primarily designed for data ingestion, querying for traces and presenting them in the UI is an example of functionality that is not in its remit. Jaeger v2 implements it as a Collector extension.</p><h3>Single BinaryÂ ğŸ§©</h3><p>Jaeger v1 provided multiple binaries for different purposes (agent, collector, ingester, query). Those binaries were hardwired to perform different functions and exposed different configuration options passed via command line. We realized that all that complexity was unnecessary in the v2 architecture as we achieve the same simply by enabling different components in the configuration file. We also did some benchmarking of executable size and noticed that if we bundle all possible Jaeger v2 components in a single binary, including ~3Mb (compressed) of UI assets, we end up with a container image of ~40Mb in size, versus ~30Mb in v1. As a result, Jaeger v2 ships just a single binary `jaeger`, and it will be configurable for different deployment roles via YAML configuration file, the same as the OpenTelemetry Collector.</p><h3>Storage Implementation ğŸ’¾</h3><p>Both Jaeger and OpenTelemetry Collector process telemetry data, but they differ in how they handleÂ it.</p><ul><li><strong>OpenTelemetry Collector:</strong> Focuses on one-way data processing (receive, process, export). To store data, it uses specific exporters for each database (e.g., Elasticsearch Exporter).</li><li><strong>Jaeger:</strong> Handles both writing and reading data, allowing queries and visualization through a UI. This requires a shared storage backend, unlike the Collectorâ€™s approach.</li></ul><p>Jaeger v2 implements a storage extension to abstract the storage layer. This allows both the query component (read path) and a generic exporter (write path) to interact with various storage backends without dedicated implementations for each. This approach provides flexibility and maintains compatibility with Jaeger v1â€™s multi-storage capabilities.</p><h3>Whatâ€™s inÂ v2</h3><ul><li>âœ… A single binary &amp; <a href=\"https://hub.docker.com/r/jaegertracing/jaeger\"><strong>Docker image</strong></a> configurable to run in different roles, such as collector, ingester, query, and a collection of <a href=\"https://github.com/jaegertracing/jaeger/tree/main/cmd/jaeger\"><strong>configuration file templates</strong></a> for theseÂ roles.</li><li>âœ… Support for all official storage backends with full backwards compatibility with JaegerÂ v1.</li><li>âœ… Support for Kafka as intermediate queue, in both OTLP and Jaeger legacy dataÂ formats.</li><li>âœ… Support for primary and archiveÂ storage.</li><li>âœ… Support for remote and adaptive head sampling.</li><li>âœ… Support for tail samplingÂ .</li><li>âœ… Service Performance Management (SPM).</li><li>âœ… Internal observability integrated with OpenTelemetry Collector configuration.</li><li>âœ… Refreshed documentation and <a href=\"https://www.jaegertracing.io/docs/next-release-v2/migration/\"><strong>v1â†’v2 migration guide</strong></a>.</li></ul><p>â–¶ï¸ Try Jaeger v2 today and check out the <a href=\"https://www.jaegertracing.io/docs/next-release-v2/getting-started/\"><strong>Getting Started</strong></a> documentation for moreÂ options:</p><pre>docker run --rm --name jaeger \\<br>  -p 5778:5778 \\<br>  -p 16686:16686 \\<br>  -p 4317:4317 \\<br>  -p 4318:4318 \\<br>  -p 14250:14250 \\<br>  -p 14268:14268 \\<br>  -p 9411:9411 \\<br>  jaegertracing/jaeger:2.1.0</pre><h3>Whatâ€™s Next</h3><p>Version 2.0.0 already supports all the core features of Jaeger, but the development will continue in 2025 to add remaining feature parity, improving performance, and enhancing the overall user experience.</p><p>The roadmap for Jaeger v2 includes the following milestones:</p><ul><li>ğŸš§ Helm Chart for JaegerÂ v2.</li><li>ğŸš§ Jaeger v2 support natively in the OpenTelemetry Operator.</li><li>ğŸ“… Upgrading storage backend implementations to Storage v2 interface to use OpenTelemetry data natively.</li><li>ğŸ“… Support ClickHouse as official storageÂ backend.</li><li>ğŸ”® Upgrading UI to use OpenTelemetry data natively.</li><li>ğŸ”® Upgrading UI to normalize dependency views.</li></ul><h3>Leveraging Mentorship ProgramsÂ ğŸ“</h3><p>The Jaeger v2 roadmap was designed to minimize the amount of changes we needed to make to the project by avoiding a big-bang approach in favor of incremental improvements to the existing code base. Yet it was still a significant amount of new development, which is often difficult to sustain for a volunteer-driven project. We were able to attract new contributors and drive the Jaeger v2 roadmap by participating in the <a href=\"https://www.jaegertracing.io/mentorship/\"><strong>mentorship programs</strong></a> run by Linux Foundation, CNCF, and Google, such as LFX Mentorship and Google Summer of Code. This has been a rewarding and mutually beneficial engagement for both the project and the participating interns. Stay tuned for our next mentorships to build out the roadmapÂ items.</p><h3>Conclusion ğŸ</h3><p>Jaeger v2 represents a significant step forward for the Jaeger project, bringing improved flexibility, extensibility, and alignment with the OpenTelemetry project. With its native OTLP ingestion, simplified deployment model, and access to OpenTelemetry Collector features, Jaeger v2 promises to provide a more efficient and scalable distributed tracing solution.</p><p>As the development of Jaeger v2 continues, we can expect to see a more robust and feature-rich system emerge. Stay tuned for updates and get ready to experience the next generation of distributed tracing with JaegerÂ v2!</p><p><em>Originally posted on </em><a href=\"https://www.cncf.io/blog/2024/11/12/jaeger-v2-released-opentelemetry-in-the-core/\"><em>CNCFÂ Blog</em></a><em>.</em></p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=09a6033d1b10\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/jaegertracing/jaeger-v2-released-09a6033d1b10\">Jaeger v2 released ğŸ‰</a> was originally published in <a href=\"https://medium.com/jaegertracing\">JaegerTracing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        },
        {
          "title": "Towards Jaeger v2  Moar OpenTelemetry!",
          "link": "https://medium.com/jaegertracing/towards-jaeger-v2-moar-opentelemetry-2f8239bee48e?source=rss----99735986d50---4",
          "guid": {
            "+content": "https://medium.com/p/2f8239bee48e",
            "+@isPermaLink": "false"
          },
          "category": [
            "distributed-tracing",
            "opentelemetry",
            "jaeger"
          ],
          "creator": "Yuri Shkuro",
          "pubDate": "Thu, 25 Jul 2024 23:44:19 GMT",
          "updated": "2024-07-25T23:44:19.069Z",
          "encoded": "<p>by <a href=\"https://shkuro.com\">Yuri Shkuro</a> and <a href=\"https://x.com/jkowall\">JonahÂ Kowall</a></p><p>Jaeger, the popular open-source distributed tracing system, is getting a major upgrade with the upcoming release of <strong>Jaeger v2</strong>. This new version is a new architecture for Jaeger backend components that utilizes <a href=\"https://opentelemetry.io/docs/collector/\">OpenTelemetry Collector</a> framework as the base and extends it with Jaegerâ€™s unique features. It promises to bring significant improvements and changes, making Jaeger more flexible, extensible, and even better aligned with the OpenTelemetry project.</p><p>In this blog post, weâ€™ll dive into the details of Jaeger v2, exploring its design, features, and benefits. Weâ€™ll also discuss the roadmap for development and what users can expect from this exciting newÂ release.</p><h3>Why OpenTelemetry Collector</h3><p>Jaeger and OpenTelemetry Collector solve different problems. Jaeger is a complete tracing platform that includes storage and the UI. OpenTelemetry Collector is usually an intermediate component in the collection pipelines that is used to receive, process, transform, and export different telemetry types. The two systems have some overlaps, for example, jaeger-agent and jaeger-collector play roles similar to what can be done with OpenTelemetry Collector, but only forÂ traces.</p><p>Historically, both Jaeger and OpenTelemetry Collector reused each otherâ€™s code. Collector supports receivers for legacy Jaeger formats implemented by importing Jaeger packages. And Jaeger reuses Collectorâ€™s OTLP receivers and OTLP-to-Jaeger data model converters. Because of this synergy, itâ€™s been our goal for a while to bring the two projectsÂ closer.</p><p>OpenTelemetry Collector has a very flexible and extensible design, which makes it easy to extend with additional components needed for Jaeger useÂ cases.</p><h3>Third Time aÂ Charm</h3><p>This is actually our third attempt to utilize the OpenTelemetry Collector framework as a basis for Jaeger v2. In the first attempt we tried to keep the Jaeger v2 configuration compatible with v1, reusing the same CLI flags, which was ultimately a mistake as it was too difficult to maintain. In the second attempt we tried to use the OpenTelemetry Collector Builder (known as ocb) to compose Jaeger v2 binary in a different repository, which also proved to be difficult to maintain.</p><p>This third attempt is much further along due to several decisions (see the <a href=\"https://docs.google.com/document/d/1s4_6VgAS7qAVp6iEm5KYvpiGw3h2Ja5T5HpKo29iv00/edit#\">RFC doc</a> for more details):</p><ul><li>We decided to break CLI configuration compatibility and embrace the OpenTelemetry Collector file configuration mechanism.</li><li>We build Jaeger v2 binary by directly importing OpenTelemetry Collector code as a library, which makes the development much easier than with ocb (although we do plan to support ocb in the future as an extension mechanism).</li><li>Weâ€™ve implemented adapter layers that allow us to reuse the existing Jaeger v1 code directly in Jaeger v2, which means we can continue evolving a single code base and do the upgrades in-place, instead of working on an incompatible fork for manyÂ months.</li></ul><h3>Features andÂ Benefits</h3><p>By aligning Jaeger v2 architecture with the OpenTelemetry Collector, we can deliver several exciting features and benefits for users, including:</p><ul><li><strong>Native OpenTelemetry processing</strong>: Jaeger v2 will natively support the <a href=\"https://opentelemetry.io/docs/specs/otel/protocol/\">OTLP data format</a>, eliminating the translation step from OTLP to Jaegerâ€™s internal data format and improving performance.</li><li><strong>Batched data processing</strong>: the OpenTelemetry Collector pipelines operate on batches of data, which can be especially important when sending data to storage backends like ClickHouse that are much more performant with batch inserts. Jaeger v2 will be able to utilize this batch-based pipeline design, in contrast to v1â€™s own internal pipeline which was designed around individual spans.</li><li><strong>Familiar developer experience</strong>: Jaeger v2 will follow the same configuration and deployment model as the OpenTelemetry Collector, providing a more consistent developer experience.</li><li><strong>Access to OpenTelemetry Collector features</strong>: Jaeger v2 will inherit all the core features of the Collector, including auth, cert reloading, internal monitoring, health checks, z-pages,Â etc.</li><li><strong>Access to OpenTelemetry Collector ecosystem</strong>: Jaeger v2 will be able to use a multitude of extensions available for OpenTelemetry Collector, such as span-to-metric connector, tail-based sampling processor, telemetry rewriting processors, PII filtering, etc. For example, we are able to reuse Kafka exporter and receiver and replicate Jaeger v1&#39;s collector/ingester deployment model without maintaining any extraÂ code.</li></ul><p>The result of v2 is less code to maintain in the Jaeger project and an assured alignment with OpenTelemetry, which is already the standard way to instrument applications and collect telemetry.</p><p>The other major benefit to users is the ability to future-proof Jaeger as OpenTelemetry evolves, to ensure Jaeger is always the first tracing system for open source users. This is likely to improve collaboration and evolution of both projects.</p><h3>Design and Architecture</h3><p>Overall, Jaeger v2 architecture is very similar to a standard OpenTelemetry Collector that has <em>pipelines</em> for receiving and processing telemetry (a pipeline encapsulates <em>receivers</em>, <em>processors</em>, and <em>exporters</em>), and <em>extensions </em>that perform functions not directly related to processing of telemetry. Jaeger v2 makes a few specific design decisions in how to use the Collector framework.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*iu_PaG5VWUVR2vRz\" /><figcaption>High level architecture of Jaeger v2Â binary</figcaption></figure><h4>Query Extension ğŸ”</h4><p>Querying for traces and presenting them in the UI is an example of functionality â€œnot directly related to telemetry processingâ€, so naturally the equivalent of v1 jaeger-query is implemented as a Collector extension in JaegerÂ v2.</p><h4>Single Binary</h4><p>Jaeger v1 provided multiple binaries for different purposes (agent, collector, ingester, query). Those binaries were hardwired to perform different functions and exposed different configuration options. We realized that all that complexity was unnecessary in v2 architecture because we can achieve the same simply by enabling different components in the configuration file. We also did some benchmarking of executable size and noticed that if we bundle all possible Jaeger v2 components in a single binary, including ~3Mb (compressed) of UI assets, we end up with all binaries being around 50Mb in size, so separating them does not bring any real benefits. As a result, Jaeger v2 will ship as just a single binary jaeger, and it will be configurable for different deployment roles via YAML configuration file, the same as the OpenTelemetry Collector.</p><h4>Storage Extension</h4><p>One significant difference between Jaeger and OpenTelemetry Collector is that the Collector is designed for one-directional data processing (receivers â†’ processors â†’ exporters), which we call the <em>write path</em>. When the write path needs to store data in a database, the traditional approach in the OpenTelemetry Collector is to implement different exporters for different backends, such as Elasticsearch Exporter. In contrast, Jaeger supports both the write path and the <em>read path</em>, via query and UI. When we combine those functions in a single binary, like the equivalent of Jaeger v1 all-in-one, the write and read paths must share the storage backend implementation, so exporter-per-storage approach does not work forÂ us.</p><p>To accommodate that, and to support Jaeger v1 existing capability of utilizing different storage backends, we abstracted the notion of a â€œstorageâ€ into Jaeger Storage Extension. The Jaeger Query Extension locates that Jaeger Storage Extension on start-up and asks it for a TraceReader. For the write path, we implemented a generic Jaeger Storage Exporter, which also consults the Storage Extension to obtain a TraceWriter implementation.</p><p>Because Query and Exporter ask for storage by name, we are able to simultaneously support different storage implementations for different purposes, for example one could configure Elasticsearch as the main trace storage, Cassandra as the archive storage, and something else as sampling strategies storage. In Jaeger v1 all storage roles had to be done in the sameÂ backend.</p><h3>Roadmap and Development</h3><p>The development of Jaeger v2 is ongoing, with several milestones planned before its general availability (GA). The alpha version is already available and supports most of Jaeger v1 functions, such as ingestion of different formats, support for the same storage backends as v1, query/UI, and all-in-one deployment. The team is working on adding remaining feature parity, improving performance, and enhancing the overall user experience.</p><p>The roadmap for Jaeger v2 includes the following milestones:</p><ul><li>âœ… Proof of concept: A single binary with memory storage and all-in-one functionality.</li><li>âœ… Storage integration: support for the same storage backends asÂ v1.</li><li>âœ… New, more comprehensive end-to-end integration tests for all storage backends.</li><li>ğŸš§ Feature parity: Kafka integration.</li><li>ğŸš§ Feature parity: Service Performance Monitoring (SPM).</li><li>ğŸ“… Prepare for Beta: release pipeline, and documentation.</li><li>ğŸ“… Prepare for GA: creating a Helm chart, Kubernetes operator, and clarifying version compatibility guarantees.</li></ul><p>Once the GA is announced, there are a few more milestones to continue improving JaegerÂ v2:</p><ul><li>ğŸš€ Upgrading UI to use OpenTelemetry data natively.</li><li>ğŸš€ Upgrading storage backend implementations to Storage v2 interface to use OpenTelemetry data natively.</li><li>ğŸš€ Support ClickHouse is official storageÂ backend.</li></ul><p>â–¶â–¶â–¶ï¸ You can try out Jaeger v2 today! We publish a Docker image (<a href=\"https://hub.docker.com/r/jaegertracing/jaeger\">https://hub.docker.com/r/jaegertracing/jaeger</a>) and we provide a collection of <a href=\"https://github.com/jaegertracing/jaeger/tree/main/cmd/jaeger\">configuration file templates</a> for different deployment modes ofÂ Jaeger.</p><h3>Leveraging Mentorship ProgramsÂ ğŸ“</h3><p>The Jaeger v2 roadmap was designed to minimize the amount of changes we need to make to the project, by avoiding big-bang approach in favor of incremental improvements to the existing code base. Yet it was still a significant amount of new development, which is often difficult to sustain for a volunteer-driven project. We were able to attract new contributors and drive the Jaeger v2 roadmap by participating in the <a href=\"https://www.jaegertracing.io/mentorship/\">mentorship programs</a> run by Linux Foundation and Google, such as LFX Mentorship and Google Summer of Code. This has been a rewarding and mutually beneficial engagement for both the project and the participating interns. We are currently proposing <a href=\"https://github.com/jaegertracing/jaeger/issues/5772\">two projects for the Fall LFXÂ term</a>.</p><h3>Conclusion</h3><p>Jaeger v2 represents a significant step forward for the Jaeger project, bringing improved flexibility, extensibility, and alignment with the OpenTelemetry project. With its native OTLP ingestion, simplified deployment model, and access to OpenTelemetry Collector features, Jaeger v2 promises to provide a more efficient and scalable distributed tracing solution.</p><p>As the development of Jaeger v2 continues, we can expect to see a more robust and feature-rich system emerge. Stay tuned for updates and get ready to experience the next generation of distributed tracing with Jaeger v2!Â ğŸ’¥</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=2f8239bee48e\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/jaegertracing/towards-jaeger-v2-moar-opentelemetry-2f8239bee48e\">Towards Jaeger v2 ğŸ’¥ğŸ’¥ğŸ’¥ Moar OpenTelemetry!</a> was originally published in <a href=\"https://medium.com/jaegertracing\">JaegerTracing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        },
        {
          "title": "Making design decisions for ClickHouse as a core storage backend in Jaeger",
          "link": "https://medium.com/jaegertracing/making-design-decisions-for-clickhouse-as-a-core-storage-backend-in-jaeger-62bf90a979d?source=rss----99735986d50---4",
          "guid": {
            "+content": "https://medium.com/p/62bf90a979d",
            "+@isPermaLink": "false"
          },
          "creator": "Ha Anh Vu",
          "pubDate": "Sun, 24 Sep 2023 20:40:51 GMT",
          "updated": "2023-09-24T23:02:07.905Z",
          "encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*nrY-UQFxXoQEjKKXDzHwSw.png\" /></figure><h3>Overview</h3><p>ClickHouse database has been used as a remote storage server for Jaeger traces for quite some time, thanks to a <a href=\"https://github.com/jaegertracing/jaeger-clickhouse\">gRPC storage plugin built by the community</a>. Lately, we have decided to make ClickHouse one of the core storage backends for Jaeger, besides Cassandra and Elasticsearch. The first step for this integration was figuring out an optimal schema design. Also, since ClickHouse is designed for batch inserts, we also needed to consider how to support that inÂ Jaeger.</p><p>There are different approaches for schema and batch insert support. So we decided to do performance benchmarks on these options, to figure out the optimal ones. In this blog, we will share our benchmark setups and results, from which we will recommend designÂ choices.</p><p>For tracing data, we found two existing schema design approaches from the <a href=\"https://github.com/jaegertracing/jaeger-clickhouse\">jaeger-clickhouse gRPC plugin</a> (or <em>plugin</em> in short) and <a href=\"https://github.com/open-telemetry/opentelemetry-collector-contrib/tree/main/exporter/clickhouseexporter\">OpenTelemetry Collectorâ€™s ClickHouse exporter</a> (or <em>exporter</em>). Both of these allow batching a configurable number of spans internally and sending batches of spans to ClickHouse. To handle batch inserts, ClickHouse also provides another option of using <a href=\"https://clickhouse.com/docs/en/optimize/asynchronous-inserts\">asynchronous insert feature</a> that we would like to try. So overall, our performance benchmarks were carried out in theseÂ steps:</p><ul><li>Step 1: Figure out which schema design approach performs better between the <em>plugin</em>â€™s and the <em>exporter</em>â€™s.</li><li>Step 2: For the higher-performance schema, figure out how it can be improved to perform evenÂ better.</li><li>Step 3: With the schema improved from step 2, try ClickHouseâ€™s asynchronous insert feature and compare the performance with internal client-side batching approach.</li></ul><p>We ran performance benchmarks on different dimensions and configs. In this blog, we will share the setups and results of the benchmarks that showed the most representative results and led to our designÂ choices.</p><p>The configs/scripts/code used to carry out the benchmarks can be found in <a href=\"https://github.com/haanhvu/clickhouse-trace-exporting-benchmark\">thisÂ repo</a>.</p><h3>Step 1: Figure out the better schema designÂ approach</h3><p>The <em>plugin</em> and the <em>exporter</em> use different database schemas. For full details of each schema, you can visit <a href=\"https://github.com/haanhvu/clickhouse-trace-exporting-benchmark/tree/main/schema\">the benchmark repo</a>. Here, we will summarize the key differences betweenÂ them:</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/9f00fedd3903b1fe0b19288499c935af/href\">https://medium.com/media/9f00fedd3903b1fe0b19288499c935af/href</a></iframe><p>To evaluate these schemas, we used these criteria (ordered by priority):</p><ul><li>Search performance</li><li>Insert performance</li><li>Compression ratio</li></ul><p>The reasons for this order of priorityÂ are:</p><ul><li>Search performance is an important metric that clearly shows the difference between twoÂ schemas.</li><li>Insert performance is also an important metric. But it is affected by other factors outside of the schema or the database, such as the number of insert threads from the clientÂ side.</li><li>Compression ratio also shows the performance of each schema, that is, how well each schema compress data and saves storage space. However, compression ratio depends very much on the cardinality of data, which is different case by case in reality. Also, a high compression ratio may also indicate high decompression cost in search queries. So there are pros but also consÂ here.</li></ul><p>The ultimate purpose of the benchmarks was to compare two schema approaches, not to examine the performance of setups, so we just aimed at a simple setup. For full technical details of the setup, please visit the <a href=\"https://github.com/haanhvu/clickhouse-trace-exporting-benchmarks/tree/main/setup\">benchmark setup directory</a>. Overall, our setup consisted of:</p><ul><li>1 instance of Jaeger Collector with jaeger-clickhouse plugin with batch size 100,000 for the <em>plugin</em>Â schema</li><li>1 instance of OpenTelemetry Collector with ClickHouse exporter with batch size 100,000 for the <em>exporter</em>Â schema</li><li>1 instance of ClickHouse single-node server for each collector</li><li>3 instances of <a href=\"https://hub.docker.com/r/jaegertracing/jaeger-tracegen\">jaeger-tracegen</a> that generated 495 millions spans in 1.2 hours to each collector. To ensure that our data was representative enough, we configured generated traces to have 11 spans per trace (1 parent span and 10 child spans), 11 attributes per child span, 97 distinct attribute keys, and 1000 distinct attribute values. (There was one benchmark where we increased the number of distinct values in attribute keys/values to 1000/10000. But the benchmark results didnâ€™t change noticeably).</li></ul><p>For benchmark results, we retrieved data from <a href=\"https://clickhouse.com/docs/en/operations/system-tables\">ClickHouse system tables</a>. The SQL queries to retrieve benchmark results can be found in the <a href=\"https://github.com/haanhvu/clickhouse-trace-exporting-benchmark/tree/main/performance-retrieval-scripts\">benchmark repo</a>.</p><p>Hereâ€™s the performance results of two benchmarked schemas (important metrics are <strong>highlighted</strong>):</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/e8acb1111de0a3f3f2b1177c3ce5cd79/href\">https://medium.com/media/e8acb1111de0a3f3f2b1177c3ce5cd79/href</a></iframe><p>Regarding <em>inserted_spans_per_sec_single_thread</em> metric, this is the estimated inserted spans per second <strong><em>in a single insert thread</em></strong>, measured by dividing the number of spans saved by the sum of insertsâ€™ durations. We needed to measure by single insert thread because the numbers of insert threads in the plugin and the exporter are different. The real inserted spans per second with multi-threaded inserts would be muchÂ higher.</p><p>With these results, we can seeÂ that:</p><ul><li>Search performance: the <em>exporter</em> schema performed better in most search queries except for searching all distinct services, searching all distinct operations, and retrieving all spans by trace ID. It seems that the search performance of the <em>plugin</em> schema was worse because it had to read more tables, hence took more time. However, the performance of searching all distinct services and searching all distinct operations of the <em>plugin</em> schema was much better, thanks to its <strong>jaeger_operations</strong> materialized view. This is one thing the <em>exporter</em> schema could learnÂ from.</li><li>Insert performance: Both used roughly equal memory per span insert. And the <em>exporter</em> schema inserted more spans per second per single insert thread than the <em>plugin</em> schema. So the <em>exporter</em> schema performed betterÂ here.</li><li>Compression ratio: The <em>plugin</em> schema had better compression ratios.</li></ul><p>As stated above, we prioritized search performance and insert performance over compression ratio, so to us the exporter schema was the better approach. The next step was trying different improvement opportunities on thisÂ schema.</p><h3>Step 2: Try improvement opportunities on OpenTelemetry Collectorâ€™s ClickHouse exporterâ€™s schema</h3><p>The first thing we tweaked was <em>ORDER BY (ServiceName, SpanName, toUnixTimestamp(Timestamp), TraceId)</em>. According to <a href=\"https://clickhouse.com/docs/en/optimize/sparse-primary-indexes\">ClickHouseâ€™s doc</a>, to ensure optimal columnsâ€™ compression ratios and search performance, the rule of thumb is to pick the columns from which data will be filtered and place lower-cardinality columns first in <em>ORDER BY.</em> So we tried adding two other filtering columns: <em>SpanAttributes</em>, <em>Duration</em>, sorting the cardinalities of the picked columns and had this new <em>ORDER BY (ServiceName, SpanName, toUnixTimestamp(Timestamp), Duration, SpanAttributes, TraceId)</em>.</p><p>With this new <em>ORDER BY</em>, the only noticeable change we observed was that the compression ratio of the Duration column doubled from 3.89 to 7.43, so adding <em>Duration</em> column to <em>ORDER BY</em> seemed like a reasonable choice. We didnâ€™t see noticeable changes in the compression or search performance of the <em>SpanAttributes</em> column. So weâ€™ll take more consideration about whether to include this column in <em>ORDER BY</em>. Also, since the cardinalities of filtering columns may be different case by case, we will allow users to configure their <em>ORDER BY</em> for optimal performance.</p><p>Next, we tried tweaking the data types of some columns since they showed better compression ratios in equivalent columns in the <em>plugin</em>Â schema:</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/196b5c18a441822a9d2fed73a77c191d/href\">https://medium.com/media/196b5c18a441822a9d2fed73a77c191d/href</a></iframe><p>With these changes in data types, we saw these changes in relatedÂ metrics:</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/3d806fbe439e9ba9e42d48f9238c369e/href\">https://medium.com/media/3d806fbe439e9ba9e42d48f9238c369e/href</a></iframe><p>From the metrics, we saw that the new data types performed better:</p><ul><li>The compression ratios were improved. Especially with <em>SpanAttributes</em>, both uncompressed size and compressed size were reduced and compression ratio was increased.</li><li>Searching all spansâ€™ information by tag/attribute took less time andÂ memory.</li></ul><p>Weâ€™ll include these changes into our designÂ choices.</p><p>(The setups and modified codebase of the OpenTelemetry Collectorâ€™s ClickHouse exporter for these schema changes can be found in the <a href=\"https://github.com/haanhvu/clickhouse-trace-exporting-benchmark/tree/main/setup/opentelemetry-collector/step2-schemaimprovements\">benchmark repo</a>).</p><h3>Step 3: Try ClickHouseâ€™s asynchronous insert feature on the furthest improvedÂ schema</h3><p>In the previous benchmarks, we tried client-side batching in both Jaeger Collector and OpenTelemetry Collector. In this step, we tried server-side batching with ClickHouseâ€™s asynchronous insert feature. Overall, we ran three benchmarks:</p><ul><li>Server-side batchingÂ alone</li><li>Server-side batching combined with small client-side batching of 10,000 spans perÂ batch</li><li>Server-side batching combined with large client-side batching of 100,000 spans perÂ batch</li></ul><p>The setups and modified codebase of the OpenTelemetry Collectorâ€™s ClickHouse exporter for these benchmarks can be found in the <a href=\"https://github.com/haanhvu/clickhouse-trace-exporting-benchmark/tree/main/setup/opentelemetry-collector/step3-asyncinsert\">benchmark repo</a>.</p><p>Hereâ€™s the benchmark results:</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/ca7aad5d51b1e312d682883d8bf665d6/href\">https://medium.com/media/ca7aad5d51b1e312d682883d8bf665d6/href</a></iframe><p>From the metrics, asynchronous insert didnâ€™t improve the rate of inserted spans, even though it decreased the memory usage per span insert. In fact, asynchronous insert in ClickHouse has a <a href=\"https://clickhouse.com/docs/en/operations/settings/settings#asynchronous-insert-settings\">wide range of settings</a> and we just tried <a href=\"https://github.com/haanhvu/opentelemetry-collector-contrib/blob/newtypes-asyncinsert/exporter/clickhouseexporter/exporter_traces.go#L241-L246\">one example of settings</a> in our benchmark. We need more experiments and feedback from users to decide whether we should support this in our storageÂ backend.</p><h3>Conclusion</h3><p>From the benchmark results, we concluded our design choices for ClickHouse as a core storage backend for Jaeger will notably consistÂ of:</p><ul><li>A single table for all dataÂ fields</li><li>A materialized view for fast services and operations retrieval to JaegerÂ UI</li><li>A materialized view of trace IDâ€™s time range for fast spans retrieval by traceÂ ID</li><li><em>ORDER BY</em> lower-cardinality filtering columns first: We will provide a default <em>ORDER BY</em> and also allow users to configure since the cardinalities of filtering columns may be different case byÂ case.</li><li>Span attributes stored with <em>Nested</em>Â type</li><li>Support for client-side batching</li></ul><p>The next step would be implementing all these choices into actual features.</p><p>Stay tuned!</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=62bf90a979d\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/jaegertracing/making-design-decisions-for-clickhouse-as-a-core-storage-backend-in-jaeger-62bf90a979d\">Making design decisions for ClickHouse as a core storage backend in Jaeger</a> was originally published in <a href=\"https://medium.com/jaegertracing\">JaegerTracing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        },
        {
          "title": "Experiment: Migrating OpenTracing-based application in Go to use the OpenTelemetry SDK",
          "link": "https://medium.com/jaegertracing/experiment-migrating-opentracing-based-application-in-go-to-use-the-opentelemetry-sdk-29b09fe2fbc4?source=rss----99735986d50---4",
          "guid": {
            "+content": "https://medium.com/p/29b09fe2fbc4",
            "+@isPermaLink": "false"
          },
          "category": [
            "opentelemetry",
            "migration",
            "jaegertracing",
            "opentracing"
          ],
          "creator": "Yuri Shkuro",
          "pubDate": "Thu, 09 Feb 2023 05:44:06 GMT",
          "updated": "2023-02-09T05:46:24.711Z",
          "encoded": "<p>TL;DR: This post explains how Jaegerâ€™s ğŸš— HotROD ğŸš— app was migrated to the OpenTelemetry SDK.</p><p>Jaegerâ€™s <a href=\"https://www.jaegertracing.io/docs/1.42/getting-started/#sample-app-hotrod\">HotROD demo</a> has been around for a few years. It was written with OpenTracing-based instrumentation, including a couple of OSS libraries for HTTP and gRPC middleware, and used Jaegerâ€™s native SDK for Go, <a href=\"https://github.com/jaegertracing/jaeger-client-go\">jaeger-client-go</a>. The latter was deprecated in 2022, so we had a choice to either convert all of the HotROD appâ€™s instrumentation to OpenTelemetry, or try the OpenTracing-bridge, which is a required part of every OpenTelemetry API / SDK. The bridge is an adapter layer that wraps an OpenTelemetry Tracer in a facade to makes it look like the OpenTracing Tracer. This way we can use the OpenTelemetry SDK in an application like HotROD that only understands the OpenTracing API.</p><p>I wanted to try the bridge solution, to minimize the code changes in the application. It is not the most efficient way, since an adapter layer incurs some performance overhead, but for a demo app it seemed like a reasonable trade-off.</p><p>The code can be found in the <a href=\"https://github.com/jaegertracing/jaeger/tree/fedeb4cab75399e4672b77efe6a067a7bd148ddf/examples/hotrod\">Jaeger repository</a> (at specific commitÂ hash).</p><h3>Setup</h3><p>First, we need to initialize the OpenTelemetry SDK and create an OpenTracing Bridge. Fortunately, I did not have to start from scratch, because there was an earlier <a href=\"https://github.com/jaegertracing/jaeger/pull/3390\">pull request #3390</a> by <a href=\"https://github.com/rbroggi\">@rbroggi</a>, which I picked up to make certain improvements. Initialization happens in pkg/tracing/init.goÂ :</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/ba947995089d22c1c24f68f2202ac1b7/href\">https://medium.com/media/ba947995089d22c1c24f68f2202ac1b7/href</a></iframe><p>In the beginning, this is a pretty vanilla OpenTelemetry SDK initialization. We create an exporter using a helper function (more on it below), and build a TracerProvider. The compliant OpenTelemetry instrumentation would use this provider to create named Tracer objects as needed, usually with distinct names reflecting the instrumentation library or the application component. However, the OpenTracing API did not have the concept of named tracers, its Tracer as a singleton, so here we create a Tracer with a blank name (in line 23) and pass it to the bridge factory that wraps it and returns an OpenTracing Tracer.</p><p>Side note: in a better-organized code there would also be some sort of close/shutdown function returned so that the caller of tracing.Init could gracefully shutdown the tracer, e.g. to flush the span buffers when stopping the application.</p><p>The original PR used the Jaeger exporter that lets the SDK export data in the Jaegerâ€™s native data format. However, last year we extended Jaeger to accept OpenTelemetryâ€™s OTLP format directly, so I decided to add a bit of flexibility and make the choice of the exporter configurable:</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/67ffc7651db87b6891d18b107029b92b/href\">https://medium.com/media/67ffc7651db87b6891d18b107029b92b/href</a></iframe><h3>Broken Traces</h3><p>At this point things should have started to work. However, the resulting traces looked likeÂ this:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*b-RsMFPGcTtpxKaL2oRMEg.png\" /><figcaption>Trace with many spans, but all coming from a single service `frontend`.</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*0XHZYX0Xwe32mMnQLdbH3w.png\" /><figcaption>Another part of the workflow captured as a different trace. It looks like there are two services here, but in fact the HotROD app simulates the `sql` and `redis` database services, itâ€™s not actually making RPC calls toÂ them.</figcaption></figure><p>Instead of one trace per request we are getting several disjoined traces. This is where <a href=\"https://github.com/rbroggi\">@rbroggi</a>â€™s PR got stuck. After some debugging I came to realize that the SDK defaults to a no-op propagation, so no trace context was sent in RPC requests between services, resulting in multiple disjoined traces for the same workflow. It was easy to fix, but it felt like an unnecessary friction in using the OpenTelemetry SDK. I also added the Baggage propagator, which we will discussÂ later.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/aaceccd029d20a9df3628341bfea94da/href\">https://medium.com/media/aaceccd029d20a9df3628341bfea94da/href</a></iframe><p>Since the Init() function is called many times by different services in the HotROD app, I only set the propagator once using sync.Once.</p><p>After this change, the traces looked better, more colorful, so I committed theÂ change.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*_DzcdeHiRp6JOG2FqA3bfg.png\" /><figcaption>A better-looking trace after â€œfixingâ€ the propagation.</figcaption></figure><h3>Traces StillÂ Broken</h3><p>However, I shouldâ€™ve paid better attention. Notice the lone span in the middle called /driver.DriverService/FindNearest. Letâ€™s take a closerÂ look:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*XZ1VM84sCFU0kjFbkiI5ig.png\" /><figcaption>A client span trying to make a gRPC request to service `driver`.</figcaption></figure><p>This span is a client-side of a gRPC request from frontend service to driver service. The latter is missing from the trace! This was a different issue with the context propagation. There was an error returned when trying to inject the context into the request headers. The instrumentation actually logged the error back into the client span, which we can see in the Logs section: Invalid Inject/Extract carrier. Unfortunately, it was difficult to spot this error without opening up the span, because the RPC itself was successful, and the instrumentation was correct in not setting the error=true span tag, which wouldâ€™ve shown in the Jaeger UI as a redÂ icon.</p><p>After a bit more digging I found the issue, which was due to a bug in the OpenTelemetry SDKâ€™s bridge implementation. You can read about it in the following GitHubÂ issue.</p><p><a href=\"https://github.com/open-telemetry/opentelemetry-go/issues/3678\">[opentracing] OT Bridge does not work with OT gRPC instrumentation Â· Issue #3678 Â· open-telemetry/opentelemetry-go</a></p><p>As of this writing, the fix if still waiting to be merged, so as a workaround I made a branch of opentracing-contrib/go-grpc and changed it to use TextMap propagation instead of HTTPHeaders, which by chance happened to work with the bridgeÂ code.</p><p>With these fixes, we were back to the â€œclassicâ€ HotRODÂ traces.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*s26BJwM-r_pP97TZ_4Nk-w.png\" /><figcaption>Full HotROD trace, as the AI overlords intended.</figcaption></figure><h3>RPC Metrics</h3><p>I was ready to call it a day, but there was one piece missing. The original Jaeger SDK initialization code had one extra featureâ€Šâ€”â€Šit was enabling the collection of RPC metrics from spans (supported only by the Go SDK in Jaeger). My original blog post, <a href=\"https://medium.com/opentracing/take-opentracing-for-a-hotrod-ride-f6e3141f7941\">Take OpenTracing for a HotROD ride</a>, had a discussion about it, so it was a shame to lose this during this upgrade. If I were upgrading to the OpenTelemetry instrumentation as well, it might have contained a metrics-oriented instrumentation, although it would somewhat miss the point of the blog post that tracing instrumentation is already sufficient in this case. Another possibility is to generate metrics from spans using a special processor in the OpenTelemetry Collector, but using the Collector is not part of the HotROD demoÂ setup.</p><p>The OpenTelemetry SDK has the notion of span processors, an abstract API invoked on all finished spans. It is similar to how the RPCMetricsObserver was implemented in the jaeger-client-go, so I did what any scrappy engineer would doâ€Šâ€”â€Šcopy &amp; paste the code from jaeger-client-go directly into the HotROD code and adopt it to implement otel.SpanProcessor. AndÂ voilÃ :</p><pre>$ curl http://127.0.0.1:8083/debug/vars | grep &#39;&quot;requests.endpoint_HTTP&#39;<br>&quot;requests.endpoint_HTTP_GET_/.error_false&quot;: 3,<br>&quot;requests.endpoint_HTTP_GET_/.error_true&quot;: 0,<br>&quot;requests.endpoint_HTTP_GET_/config.error_false&quot;: 4,<br>&quot;requests.endpoint_HTTP_GET_/config.error_true&quot;: 0,<br>&quot;requests.endpoint_HTTP_GET_/customer.error_false&quot;: 4,<br>&quot;requests.endpoint_HTTP_GET_/customer.error_true&quot;: 0,<br>&quot;requests.endpoint_HTTP_GET_/debug/vars.error_false&quot;: 5,<br>&quot;requests.endpoint_HTTP_GET_/debug/vars.error_true&quot;: 0,<br>&quot;requests.endpoint_HTTP_GET_/dispatch.error_false&quot;: 4,<br>&quot;requests.endpoint_HTTP_GET_/dispatch.error_true&quot;: 0,<br>&quot;requests.endpoint_HTTP_GET_/route.error_false&quot;: 40,<br>&quot;requests.endpoint_HTTP_GET_/route.error_true&quot;: 0,</pre><h3>Baggage</h3><p>As I was looking through the metrics in HotROD, I realized there was another area I neglected. These sections in the expvar output were not supposed to beÂ empty:</p><pre>$ curl http://127.0.0.1:8083/debug/vars | grep route.calc.by<br>&quot;route.calc.by.customer.sec&quot;: {},<br>&quot;route.calc.by.session.sec&quot;: {}</pre><p>These measures require baggage to work. The term â€œbaggageâ€ was introduced in the academia (<a href=\"https://www.usenix.org/conference/atc16/technical-sessions/presentation/mace\">Jonathan Mace <em>et al., </em>SOSP 2015 Best Paper Award</a>). It refers to a <a href=\"https://medium.com/jaegertracing/embracing-context-propagation-7100b9b6029a\">general-purpose context propagation mechanism</a>, which can be used to carry both the tracing context and any other contextual metadata across the distributed workflow execution. The HotROD app demonstrates a number of capabilities that require baggage propagation, and they were all completely broken after upgrading to OpenTelemetry SDKÂ ğŸ˜­.</p><p>The first thing that broke was propagation of baggage from the web UI. HotROD does not start the trace in the browser, only in the backend. The Jaeger SDK had a feature that allowed it to accept baggage from the incoming request even when there was no incoming tracing context. Internally the Jaeger SDK achieved this by returning an â€œinvalidâ€ SpanContext from the Extract method where the trace ID / span ID were blank, but the baggage was present. Digging through the OpenTracing Bridge code I found that it returns an error in this case. This could probably be fixed there, but I decided to add a workaround directly to HotROD where I used the OpenTelemetryâ€™s Baggage propagator to extract the baggage from the request manually and then copy it into theÂ span.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/ab6775e1bef5ed4e4561486174c6754b/href\">https://medium.com/media/ab6775e1bef5ed4e4561486174c6754b/href</a></iframe><p>I trimmed down the code example above a bit to only show relevant parts. The otelBaggageExtractor function creates a middleware that manually extracts the baggage into the current Context. Then the instrumentation library nethttp is given a span observer (invoked after the server span is created) which copies the baggage from the context into the span. This functionality is only needed at the root span, because once the trace context is propagated through the workflow, the Bridge correctly propagates the baggage as well (remember that I registered Baggage propagator as a global propagator in the Init function, as shown in the earlier code snippet). I was actually pleasantly surprised that the maintainers were able to achieve that, because the OpenTracing API operates purely on Span objects, not on the Context, while in OpenTelemetry the baggage is carried in the Context, a lower logicalÂ level.</p><p>One other small change I had to make was to change the web UI to use the baggage header (per W3C standard), instead of the jaeger-baggage header that was recognized by the JaegerÂ SDK.</p><p>Strictly speaking, these were all the changes I had to make to the HotROD code to make the baggage work. Yet, it didnâ€™t work. Some baggage values were correctly propagated, but others were missing. After more digging I found several places where it was silently dropped on the floor because of some (misplaced, in my opinion) validations in the baggage and the bridge/opentracing packages in the OpenTelemetry SDK. The ticket below explains the issue in moreÂ details.</p><p><a href=\"https://github.com/open-telemetry/opentelemetry-go/issues/3685\">Baggage not working with OpenTracing Bridge Â· Issue #3685 Â· open-telemetry/opentelemetry-go</a></p><p>Running against a patched version of OpenTelemetry SDK yielded the desired behavior and the baggage-reliant functionality was restored. I was getting performance metrics grouped by baggageÂ values:</p><pre>$ curl http://127.0.0.1:8083/debug/vars | grep route.calc.by<br>&quot;route.calc.by.customer.sec&quot;: {<br>  &quot;Amazing Coffee Roasters&quot;: 0.9080000000000004, <br>  &quot;Japanese Desserts&quot;: 1.0490000000000002, <br>  &quot;Rachel&#39;s Floral Designs&quot;: 1.0090000000000003, <br>  &quot;Trom Chocolatier&quot;: 1.0000000000000004<br>},<br>&quot;route.calc.by.session.sec&quot;: {<br>  &quot;2885&quot;: 1.4760000000000002, <br>  &quot;5161&quot;: 2.4899999999999993<br>}</pre><p>And the mutex instrumentation was able to capture IDs of multiple transactions in the queue (see the <a href=\"https://medium.com/opentracing/take-opentracing-for-a-hotrod-ride-f6e3141f7941\">original blog post</a> for explanation of thisÂ one):</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*lxeVafTIXsC7PVzkaOLIhA.png\" /><figcaption>Logs show a transactions blocked on three other transactions.</figcaption></figure><h3>Summary</h3><p>Overall, the migration required fairly minimal amount of changes to the code, mostly because I chose to reuse the existing OpenTracing instrumentation and only swap the SDK from Jaeger to OpenTelemetry. The most friction with the migration was due to a couple of bugs in the OpenTelemetry Bridge code (and likely one place in the baggage package). This only leads me to believe that the baggage functionality is not yet widely used, especially when someone uses the OpenTracing instrumentation with a bridge to OpenTelemetry, so it is likely I just ran into a bunch of the early adopterÂ issues.</p><p>At this point I am interested in taking the next step and doing a full migration of HotROD to OpenTelemetry (or help reviewing if someone wants to volunteer!) It could make a complementary Part 2 to this post to describe how thatÂ goes.</p><p>There is also a possible Part 3 involving a no-less interesting migration to the OpenTelemetry Metrics. Right now all of the Jaeger code base is using an internal abstraction for metrics backed by the Prometheus SDK.</p><p>Stay tuned.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=29b09fe2fbc4\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/jaegertracing/experiment-migrating-opentracing-based-application-in-go-to-use-the-opentelemetry-sdk-29b09fe2fbc4\">Experiment: Migrating OpenTracing-based application in Go to use the OpenTelemetry SDK</a> was originally published in <a href=\"https://medium.com/jaegertracing\">JaegerTracing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        },
        {
          "title": "Better alignment with OpenTelemetry by focusing on OTLP",
          "link": "https://medium.com/jaegertracing/better-alignment-with-opentelemetry-by-focusing-on-otlp-f3688939073f?source=rss----99735986d50---4",
          "guid": {
            "+content": "https://medium.com/p/f3688939073f",
            "+@isPermaLink": "false"
          },
          "category": "opentelemetry",
          "creator": "Yuri Shkuro",
          "pubDate": "Thu, 03 Nov 2022 18:40:39 GMT",
          "updated": "2022-11-03T18:12:55.688Z",
          "encoded": "<p>TL;DR: proposal (and a <a href=\"https://forms.gle/aUuJg5DQwNzncJ4s8\">survey</a>) to deprecate native Jaeger exporters in OpenTelemetry SDKs in favor of OTLP exporters.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*iFJFYZsdPvaFuwaAoZ1HRQ.jpeg\" /><figcaption>Photo by <a href=\"https://unsplash.com/@miquel_parera_mila?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Miquel Parera</a> onÂ <a href=\"https://unsplash.com/?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash</a></figcaption></figure><p>This is a re-post from the <a href=\"https://opentelemetry.io/blog/2022/jaeger-native-otlp/\">OpenTelemetry blogÂ article</a>.</p><p>By <a href=\"https://github.com/breedx-splk\"><strong>Jason Plumb</strong></a><strong> (Splunk)</strong> | Thursday, November 03,Â 2022</p><p>Back in May of 2022, the Jaeger project <a href=\"https://medium.com/jaegertracing/introducing-native-support-for-opentelemetry-in-jaeger-eb661be8183c\">announced native support for the OpenTelemetry Protocol</a> (OTLP). This followed a <a href=\"https://twitter.com/YuriShkuro/status/1455170693197402119\">generous deprecation cycle</a> for the Jaeger client libraries across many languages. With these changes, OpenTelemetry users are now able to send traces into Jaeger with industry-standard OTLP, and the Jaeger client library repositories have been finally archived.</p><p>We intend to <strong>deprecate Jaeger exporters from OpenTelemetry</strong> in the near future, and are looking for your feedback to determine the length of the deprecation phase. The best way to provide feedback is by <a href=\"https://forms.gle/aUuJg5DQwNzncJ4s8\">filling out a 4-question survey</a> or commenting on <a href=\"https://github.com/open-telemetry/opentelemetry-specification/pull/2858\">the existing draft pullÂ request</a>.</p><h3>OpenTelemetry Support</h3><p>This interoperability is a wonderful victory both for Jaeger users and for OpenTelemetry users. However, weâ€™re not done yet. The OpenTelemetry specification still requires support for Jaeger client exporters across languages.</p><p>This causes challenges for both Jaeger users and OpenTelemetry maintainers:</p><ol><li><strong>Confusing Choices: </strong>Currently, users are faced with a choice of exporter (Jaeger or OTLP), and this can be a source of confusion. A user might be inclined, when exporting telemetry to Jaeger, to simply choose the Jaeger exporter because the name matches (even though Jaeger now actively encourages the use of OTLP).<br>If we can eliminate this potentially confusing choice, we can improve the user experience and continue standardizing on a single interoperable protocol. We love it when things â€œjust workâ€ out of theÂ box!</li><li><strong>Maintenance and duplication: </strong>Because the Jaeger client libraries are now archived, they will not receive updates (including security patches). To continue properly supporting Jaeger client exporters, OpenTelemetry authors would be required to re-implement some of the functionality it had previously leveraged from the Jaeger clients.<br>Now that Jaeger supports OTLP, this feels like a step backwards: It results in an increased maintenance burden with very littleÂ benefit.</li></ol><h3>User Impact</h3><p>The proposal is to deprecate the following exporters from OpenTelemetry in favor of using native OTLP intoÂ Jaeger:</p><ul><li>Jaeger Thrift overÂ HTTP</li><li>Jaeger Protobuf viaÂ gRPC</li><li>Jaeger Thrift overÂ UDP</li></ul><p>In addition to application configuration changes, there could be other architectural considerations. HTTP and gRPC should be straightforward replacements, although it may require exposing ports 4317 and 4318 if they are not already accessible.</p><p>Thrift over UDP implies the use of the <a href=\"https://www.jaegertracing.io/docs/1.24/architecture/#agent\">Jaeger Agent</a>. Users with this deployment configuration will need to make a slightly more complicated change, typically one of the following:</p><ol><li>Direct ingest. Applications will change from using Thrift+UDP to sending OTLP traces directly to their jaeger-collector instance. This may also have sampling implications.</li><li>Replacing the Jaeger Agent with a sidecar <a href=\"https://github.com/open-telemetry/opentelemetry-collector\">OpenTelemetry Collector</a> instance. This could have sampling implications and requires changes to your infrastructure deployment code.</li></ol><h3>Intent to Deprecateâ€Šâ€”â€ŠWeâ€™d Like Your Feedback!</h3><p>In order to better support users and the interop between OpenTelemetry and Jaeger, we intend to deprecate and eventually remove support for Jaeger client exporters / Jaeger native data format in OpenTelemetry.</p><p>We would like your feedback! We want to hear from users who could be impacted by this change. To better make a data-informed decision, <a href=\"https://forms.gle/aUuJg5DQwNzncJ4s8\">we have put together a short 4-question survey</a>.</p><p>Your input will help us to choose how long to deprecate beforeÂ removal.</p><p>A <a href=\"https://github.com/open-telemetry/opentelemetry-specification/pull/2858\">draft PR has been created in the specification</a> to support this deprecation. If would like to contribute and provide feedback, visit the link above and add some comments. We want to hear fromÂ you.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f3688939073f\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/jaegertracing/better-alignment-with-opentelemetry-by-focusing-on-otlp-f3688939073f\">Better alignment with OpenTelemetry by focusing on OTLP</a> was originally published in <a href=\"https://medium.com/jaegertracing\">JaegerTracing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        },
        {
          "title": "Introducing native support for OpenTelemetry in Jaeger",
          "link": "https://medium.com/jaegertracing/introducing-native-support-for-opentelemetry-in-jaeger-eb661be8183c?source=rss----99735986d50---4",
          "guid": {
            "+content": "https://medium.com/p/eb661be8183c",
            "+@isPermaLink": "false"
          },
          "category": [
            "opentelemetry",
            "jaeger",
            "distributed-tracing"
          ],
          "creator": "Yuri Shkuro",
          "pubDate": "Mon, 30 May 2022 22:45:00 GMT",
          "updated": "2022-05-31T18:17:15.393Z",
          "encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1000/1*jJm7BHkMListmocakAMcqA.png\" /></figure><p>The latest <a href=\"https://github.com/jaegertracing/jaeger/releases/tag/v1.35.0\">Jaeger v1.35 release</a> introduced the ability to receive OpenTelemetry trace data via the <a href=\"https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/protocol/otlp.md\">OpenTelemetry Protocol (OTLP)</a>, which all OpenTelemetry SDKs are required to support. This is a follow-up to the previous <a href=\"https://twitter.com/YuriShkuro/status/1455170693197402119\">announcement</a> to retire Jaegerâ€™s â€œclassicâ€ client libraries.</p><p>With this new capability, it is no longer necessary to use the Jaeger exporters with the OpenTelemetry SDKs, or to run the OpenTelemetry Collector in front of the Jaeger backend. Using the OTLP exporter, the SDKs can be configured to send the data directly to the Jaeger backend. The OTLP receiver accepts data via gRPC and HTTP endpoints (gRPC mode had an issue that was patched inÂ 1.35.1).</p><h3>Primer</h3><p>Letâ€™s see this functionality in action. First, start the Jaeger all-in-one as described in the <a href=\"https://www.jaegertracing.io/docs/latest/getting-started/\">Getting Started documentation</a>:</p><pre>docker run --name jaeger \\<br>  -e COLLECTOR_OTLP_ENABLED=true \\<br>  -p 16686:16686 \\<br>  -p 4317:4317 \\<br>  -p 4318:4318 \\<br>  jaegertracing/all-in-one:1.35</pre><p>Notice that compared to the previous releases:</p><ol><li>There are two more ports added to the export list, 4317 and 4318, used by the OTLP receiver to listen for gRPC and HTTP connections.</li><li>The OTLP receiver must be enabled via COLLECTOR_OTLP_ENABLED=true.</li><li>We removed the other ports that are not relevant to thisÂ example.</li></ol><p>When the Jaeger backend is starting, you should see these two logÂ lines:</p><pre>{... &quot;msg&quot;:&quot;Starting GRPC server on endpoint 0.0.0.0:4317&quot;}<br>{... &quot;msg&quot;:&quot;Starting HTTP server on endpoint 0.0.0.0:4318&quot;}</pre><p>As usual, the Jaeger UI can be accessed at <a href=\"http://localhost:16686/\">http://localhost:16686/</a>.</p><p>Now letâ€™s use a simple Python program that configures the OpenTelemetry SDK with OTLPSpanExporter and generates a single-span trace.</p><iframe src=\"\" width=\"0\" height=\"0\" frameborder=\"0\" scrolling=\"no\"><a href=\"https://medium.com/media/5288cb4b7a19d2a79483198694ff3960/href\">https://medium.com/media/5288cb4b7a19d2a79483198694ff3960/href</a></iframe><p>Run this program asÂ follows:</p><pre>pip install -r requirements.txt<br>OTEL_SERVICE_NAME=primer python3 basic_trace.py</pre><p>If we now refresh the Search screen in the Jaeger UI, the Services dropdown should contain the service primer(notice that we pass this service name to the SDK via an environment variable) and the trace from this service should look likeÂ this:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Fb8xit2l7SxYMyKhIRrBGg.png\" /><figcaption>Sample trace submitted viaÂ OTLP.</figcaption></figure><p>The OTLP receiver can be further customized by a number of flags starting with --collector.otlp.*, available via the help command in the collector and all-in-one binaries. These flags allow changing the port numbers for the two OTLP servers, configuring TLS, and changing some other parameters like the max message size and keep-alive.</p><h3>Limitations</h3><p>There are a few caveats with the existing implementation:</p><ul><li>If your application exports both traces and metrics using OTLP, then you would still need to run the OpenTelemetry Collector, because the Jaeger collector can only accept the tracing portion of OTLP data. Alternatively, you may configure the SDK with two OTLP exporters pointing to different backends.</li><li>Not all the options supported by the OTLP receiver in the OpenTelemetry Collector are supported by the JaegerÂ backend.</li><li>Only the Jaeger collector supports the new OTLP receiver. The Jaeger agent only supports the â€œclassicâ€ Jaeger formats. If your deployment requires a local agent, we recommend running the OpenTelemetry Collector in that capacity.</li></ul><h3>Try itÂ out</h3><p>As usual, we are interested in the communityâ€™s feedback about this new feature; please comment <a href=\"https://www.jaegertracing.io/get-in-touch/\">in the chat room</a> or open anÂ issue.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=eb661be8183c\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/jaegertracing/introducing-native-support-for-opentelemetry-in-jaeger-eb661be8183c\">Introducing native support for OpenTelemetry in Jaeger</a> was originally published in <a href=\"https://medium.com/jaegertracing\">JaegerTracing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        },
        {
          "title": "Jaeger Tracing: A Friendly Guide for Beginners",
          "link": "https://medium.com/jaegertracing/jaeger-tracing-a-friendly-guide-for-beginners-7b53a4a568ca?source=rss----99735986d50---4",
          "guid": {
            "+content": "https://medium.com/p/7b53a4a568ca",
            "+@isPermaLink": "false"
          },
          "category": [
            "opentelemetry",
            "observability",
            "distributed-tracing",
            "jaeger"
          ],
          "creator": "Team Aspecto",
          "pubDate": "Sun, 27 Feb 2022 20:39:38 GMT",
          "updated": "2022-02-27T20:39:37.929Z",
          "encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*T1FPdT_GcTo_2loOgrLyUA.png\" /></figure><p>Written by <a href=\"https://twitter.com/thetomzach\">@thetomzach</a> @Â <a href=\"https://www.aspecto.io/?utm_source=jaeger-medium&amp;utm_medium=post&amp;utm_campaign=jaeger-tracing-the-ultimate-guide\">Aspecto</a>.</p><p>In this guide, youâ€™ll learn what Jaeger tracing is, what distributed tracing is, and how to set it up in your system. Weâ€™ll go over Jaegerâ€™s UI and touch on advanced concepts such as sampling and deploying in production.</p><p>Youâ€™ll leave this guide knowing how to create spans with OpenTelemetry and send them to Jaeger tracing for visualization. All that, fromÂ scratch.</p><h3>What is Distributed Tracing? Introduction</h3><p>Before we dive into explaining everything you need to know from 0 to 100 about Jaeger tracing, itâ€™s important to understand the umbrella term that Jaeger is part ofâ€Šâ€”â€Šdistributed tracing.</p><p>In the world of microservices, most issues occur due to networking issues and the relations between the different microservices. A distributed architecture (as opposed to a monolith) makes it a lot harder to get to the root of an issue. To resolve these issues, we need to see which service sent what parameters to another service or a component (a DB, queue,Â etc.).</p><p>Distributed tracing helps us achieve just that by enabling us to collect data from the different parts of our system, to enable this desired observability into our system. You can think of it as â€˜call-stacksâ€™ for distributed services. In addition, traces are a visual tool, allowing us to visualize our system to better understand the relationships between services, making it easier to investigate and pinpointÂ issues.</p><h3><strong>What is JaegerÂ Tracing?</strong></h3><p>Now that you know what distributed tracing is, we can safely talk about Jaeger. Jaeger is an open-source distributed tracing platform created by Uber back in 2015. It consists of instrumentation SDKs, a backend for data collection and storage, a UI for visualizing the data, and Spark/Flink framework for aggregate trace analysis.</p><p>The Jaeger data model is compatible with OpenTracingâ€Šâ€”â€Šwhich is a specification that defines how the collected tracing data would look, as well as libraries of implementations in different languages (more on OpenTracing and OpenTelemetry later).</p><p>As most other distributed tracing systems, Jaeger works with spans and traces, as defined in the OpenTracing specification.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*DzjXpBSuNiyCFcYq\" /></figure><p>A span represents a unit of work in an application(HTTP request, call to a DB, etc) and is Jaegerâ€™s most basic unit of work. A span must have an operation name, start time, and duration.</p><p>A trace is a collection/list of spans connected in a parent/child relationship (and can also be thought of as a directed acyclic graph of spans). Traces specify how requests are propagated through our services and other components.</p><h3>Jaeger Tracing Architecture</h3><p>Hereâ€™s what Jaeger architecture looksÂ like</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*xIdm2tN5PkOTJHy-\" /></figure><p>It consists of a few parts, all of which I explainÂ below:</p><ul><li><strong>Instrumentation SDKs: </strong>libraries that are integrated into applications and frameworks to capture tracing data. Historically the Jaeger project supported its own clients libraries written in various programming languages. They are now being deprecated in favor of OpenTelemetry (again, more on thatÂ later).</li><li><strong>Jaeger Agent:</strong> Jaeger agent is a network daemon that listens for spans received from the Jaeger client over UDP. It gathers batches of them and then sends them together to the collector. The agent is not required if the SDKs are configured to send the spans directly to the collector.</li><li><strong>Jaeger Collector:</strong> The Jaeger collector is responsible for receiving traces from the Jaeger agent, performing validations and transformations, and saving them to the selected storage backends.</li><li><strong>Storage Backends: </strong>Jaeger supports various storage backends to store the spans. Supported storage backends are In-Memory, Cassandra, Elasticsearch, and Badger (for single-instance collector deployments).</li><li><strong>Jaeger Query: </strong>This is a service responsible for retrieving traces from the Jaeger storage backend and making them accessible for the JaegerÂ UI.</li><li><strong>Jaeger UI:</strong> a React application that lets you visualize the traces and analyze them. Useful for debugging systemÂ issues.</li><li><strong>Ingester:</strong> The ingester is relevant only if we use Kafka as a buffer between the collector and the storage backend. It is responsible for receiving data from Kafka and ingesting it into the storage backend. More info can be found in the <a href=\"https://www.jaegertracing.io/docs/1.30/architecture/#ingester\">official Jaeger TracingÂ docs</a>.</li></ul><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*6Pjtk8IgfVpfQp2F\" /></figure><h3>Running Jaeger locally usingÂ Docker</h3><p>Jaeger comes with a ready-to-use <strong>all-in-one</strong> Docker image that contains all the components necessary for Jaeger toÂ run.</p><p>Itâ€™s really simple to get it up and running on your localÂ machine:</p><pre>docker run -d --name jaeger \\<br>  -e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \\<br>  -p 5775:5775/udp \\<br>  -p 6831:6831/udp \\<br>  -p 6832:6832/udp \\<br>  -p 5778:5778 \\<br>  -p 16686:16686 \\<br>  -p 14250:14250 \\<br>  -p 14268:14268 \\<br>  -p 14269:14269 \\<br>  -p 9411:9411 \\<br>  jaegertracing/all-in-one:1.30</pre><p>Then you can simply open the jaeger UI on <a href=\"http://localhost:16686/\">http://localhost:16686</a>.</p><h3>Jaeger Tracing and OpenTelemetry</h3><p>Yes, youâ€™re right. I did mention before that Jaegerâ€™s data model is compatible with the OpenTracing specification. You may already know that OpenTracing and OpenCensus have merged to form OpenTelemetry and are wondering why does Jaeger use OpenTracing and if you can use OpenTelemetry to report to JaegerÂ instead.</p><p>As to why Jaeger uses OpenTracingâ€Šâ€”â€Šwell, the reason is that Jaeger existed from before the above-mentioned merger.</p><p>To get a full understanding of OpenTelemetry, what is it, its components, and how you can use with, <a href=\"https://www.aspecto.io/blog/what-is-opentelemetry-the-infinitive-guide/?utm_source=jaeger-medium&amp;utm_medium=post&amp;utm_campaign=jaeger-tracing-the-ultimate-guide\"><strong>read thisÂ guide</strong></a>.</p><h3>Deprecation of Jaeger Client in favor of OpenTelemetry Distro:</h3><p>I also mentioned that Jaeger clients are now deprecating.</p><p>You can find more info about this deprecation <a href=\"https://github.com/jaegertracing/jaeger/issues/3362\">here</a>, but essentially the idea is that you should now use the OpenTelemetry SDK in the programming language of your choice, alongside a Jaeger exporter.</p><p>This way created spans would be converted to a format Jaeger knows how to work with, passing all the way through to the Jaeger collector and then to the storageÂ backend.</p><p>At the time of writing this, the OpenTelemetry collector is not considered a replacement for the Jaeger collector [<a href=\"https://github.com/jaegertracing/jaeger/milestone/12\">1</a>]. In the future, the Jaeger collector will be able to receive OTLP, the OpenTelemetry native dataÂ format.</p><p>If you canâ€™t wait and want to try using the OpenTelemetry collector with Jaeger nowâ€Šâ€”â€Šsee <a href=\"https://medium.com/jaegertracing/jaeger-embraces-opentelemetry-collector-90a545cbc24\">thisÂ guide</a>.</p><h3>Jaeger Tracing PythonÂ Example</h3><p>Here is a Python example of creating spans and sending them to Jaeger. Note that you could also use automatic instrumentations and still use the Jaeger exporter (assuming youâ€™re running Jaeger locally like shownÂ above):</p><pre># jaeger_tracing.py<br>from opentelemetry import trace<br>from opentelemetry.exporter.jaeger.thrift import JaegerExporter<br>from opentelemetry.sdk.resources import SERVICE_NAME, Resource<br>from opentelemetry.sdk.trace import TracerProvider<br>from opentelemetry.sdk.trace.export import BatchSpanProcessor</pre><pre>trace.set_tracer_provider(<br>   TracerProvider(<br>       resource=Resource.create({SERVICE_NAME: &quot;my-hello-service&quot;})<br>   )<br>)</pre><pre>jaeger_exporter = JaegerExporter(<br>   agent_host_name=&quot;localhost&quot;,<br>   agent_port=6831,<br>)</pre><pre>trace.get_tracer_provider().add_span_processor(<br>   BatchSpanProcessor(jaeger_exporter)<br>)</pre><pre>tracer = trace.get_tracer(__name__)</pre><pre>with tracer.start_as_current_span(&quot;rootSpan&quot;):<br>   with tracer.start_as_current_span(&quot;childSpan&quot;):<br>           print(&quot;Hello world!&quot;)</pre><p>This is what they would look like in the JaegerÂ UI:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*BYbo6cI12ePWQ_OF\" /></figure><p>To learn how to hands-on use OpenTelemetry in Python from scratch, <a href=\"https://www.aspecto.io/blog/getting-started-with-opentelemetry-python/?utm_source=jaeger-medium&amp;utm_medium=post&amp;utm_campaign=jaeger-tracing-the-ultimate-guide\"><strong>read thisÂ guide</strong></a><strong>.</strong></p><h3>Jaeger Tracing UIÂ Review</h3><p>The Jaeger UI is a powerful tool for us to debug and understand our distributed servicesÂ better.</p><p>Hereâ€™s what you need to know aboutÂ it:</p><p>The searchÂ pane:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/479/0*TiHLXzFCPMCyXoHM\" /></figure><p>You can use the search pane to search for traces with specific properties: which service they come from, what operation was made, specific tags that were included within the trace (for example, the http status code), how long in the past to look for and result amount limiting.</p><p>When youâ€™re done defining your search in this pane, click on FindÂ Traces.</p><p>The search resultsÂ section:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*E8Oe-476BxUPjmU2\" /></figure><p>In this example, I chose to query the jaeger-query service. I can see my traces on a timeline or as a list. Click on the desired trace to drill down intoÂ it.</p><p>The specific traceÂ view:</p><p>When you find a specific trace where you think there might be an issue and click on it, youâ€™d see a screen that looks likeÂ this:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/0*FlqtsSJIuUXSb8yz\" /></figure><p>Here you can find specific information about execution times, which calls were made, their durations, specific properties like http status code, route path (in the case of an http call), andÂ more.</p><p>Feel free to play around and investigate for yourself with your actualÂ data.</p><h3><strong>Advanced Concepts: Sampling</strong></h3><p>Sampling is a complex topic by itself. in Jaeger the sampling decisions are always made in the SDK, via head-based sampling.</p><h4>Sampling Strategies in theÂ SDK</h4><p>The (deprecating) Jaeger SDKs have 4 samplingÂ modes:</p><ul><li>Remote: the default, and is used to tell the Jaeger SDK that sampling strategy is controlled by the JaegerÂ backend.</li><li>Constant: either take all traces or take none. Nothing in between. Receives 1 for all and 0 forÂ none</li><li>Rate limiting: choose how many traces would be sampled perÂ second.</li><li>Probabilistic: choose a percentage of traces that would be sampled, for exampleâ€Šâ€”â€Šchoose 0.1 to have 1 of each 10 traces to beÂ sampled.</li></ul><h4>Remote sampling</h4><p>If we choose to enable remote sampling, the Jaeger collector becomes responsible for figuring out which sampling strategy an SDK in each service should be using. The operators have two ways to configure the collector: with a sampling strategies configuration file, or with adaptive sampling.</p><p>Configuration fileâ€Šâ€”â€Šyou give the collector a path to a file that contains the per-service and pre-operation sampling configuration.</p><p>Adaptive samplingâ€Šâ€”â€Šlet Jaeger learn the amount of traffic each endpoint receives and calculate the most appropriate rate for that endpoint. Note that at the time of writing only Memory and Cassandra backends supportÂ this.</p><p>More info on Jaeger sampling can be found here: <a href=\"https://www.jaegertracing.io/docs/1.30/sampling/\">https://www.jaegertracing.io/docs/latest/sampling/</a></p><h3>Jaeger Tracing Production Deployment</h3><h4>All-in-one or separate containers?</h4><p>Jaeger all-in-one is a pre-built Docker image containing all the Jaeger components needed to get up and running quickly with Jaeger tracing by launching a singleÂ command.</p><p>A lot of people (including my past self) ask themselves whatâ€™s the correct way to launch Jaeger in production. If itâ€™s safe to use Jaeger all-in-one in production, etc. While at the time of writing I could not find any official answer to use or not to use it, I think the right answer isâ€Šâ€”â€Šyou could, but you probably shouldnâ€™t. Using it as in production means you have a single source of failure which is not distributed. Theoretically, an issue even with the Jaeger UI might crush the entire container and you wouldnâ€™t be able to receive critical spans created by yourÂ system.</p><p>The best way to go about this would be to run each Jaeger component separately, without the all-in-one.</p><h3>Mastering OpenTelemetry and Distributed Tracing</h3><p>Jaeger is a distributed tracing beast and the leading open source project for tracing visualization. OpenTelemetry is becoming the industry standard for tracing instrumentation, making it a good place to start learning and implementing traces. To get started with OpenTelemetry, check out this free, vendor-neutral <a href=\"https://www.aspecto.io/opentelemetry-bootcamp/?utm_source=jaeger-medium&amp;utm_medium=post&amp;utm_campaign=jaeger-tracing-the-ultimate-guide\">OpenTelemetry Bootcamp.</a></p><p>The <a href=\"https://www.aspecto.io/opentelemetry-bootcamp/?utm_source=jaeger-medium&amp;utm_medium=post&amp;utm_campaign=jaeger-tracing-the-ultimate-guide\">Bootcamp</a>Â includes</p><ul><li>Episode 1: OpenTelemetry Fundamentals</li><li>Episode 2: Integrate Your Code (logs, metrics, andÂ traces)</li><li>Episode 3: Deploy to Production + Collector</li><li>Episode 4: Sampling and Dealing with HighÂ Volumes</li><li>Episode 5: Custom Instrumentation</li><li>Episode 6: Testing with OpenTelemetry</li></ul><p>If you have any questions, feel free to reach out to me on Twitter <a href=\"https://twitter.com/thetomzach\">@thetomzach</a> and to join our <a href=\"https://cloud-native.slack.com/messages/opentelemetry-bootcamp\">#OpenTelemetry-Bootcamp</a> slack channel to be on top of whatâ€™s happening in observability.</p><h3>Jaeger TracingÂ Glossary</h3><p><strong>Span</strong>â€Šâ€”â€Ša representation of a unit of work (action/operation) that occurs in our system; an HTTP request or a database operation that spans over time (start at X and has a duration of Y milliseconds). Usually, it will be the parent and/or child of anotherÂ span.</p><p><strong>Trace</strong>â€Šâ€”â€Ša tree/list of spans representing the progression of requests as it is handled by the different services and components in our system. For example, sending an API call to user-service resulted in a DB query to users-db. They are â€˜call-stacksâ€™ for distributed services.</p><p><strong>Observability</strong>â€Šâ€”â€Ša measure of how well we can understand the internal states of a system based on its external outputs. When you have logs, metrics, and traces you have the â€œ3 pillars of observabilityâ€.</p><p><strong>OpenTelemetry</strong>â€Šâ€”â€ŠOpenTelemetry is an open-source project at CNCF (Cloud Native Computing Function) that provides a collection of tools, APIs, and SDKs. OpenTelemetry enables the automated collection and generation of traces, logs, and metrics with a single specification.</p><p><strong>OpenTracing</strong>â€Šâ€”â€Šan open-source project for distributed tracing. It was deprecated and â€œmergedâ€ into OpenTelemetry. OpenTelemetry offers backward compatibility for OpenTracing.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=7b53a4a568ca\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/jaegertracing/jaeger-tracing-a-friendly-guide-for-beginners-7b53a4a568ca\">Jaeger Tracing: A Friendly Guide for Beginners</a> was originally published in <a href=\"https://medium.com/jaegertracing\">JaegerTracing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        },
        {
          "title": "Adaptive Sampling in Jaeger",
          "link": "https://medium.com/jaegertracing/adaptive-sampling-in-jaeger-50f336f4334?source=rss----99735986d50---4",
          "guid": {
            "+content": "https://medium.com/p/50f336f4334",
            "+@isPermaLink": "false"
          },
          "category": [
            "adaptive-sampling",
            "sampling",
            "jaeger"
          ],
          "creator": "Yuri Shkuro",
          "pubDate": "Mon, 17 Jan 2022 15:44:58 GMT",
          "updated": "2022-01-17T15:44:58.783Z",
          "encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*NKMqp96KpCm1hyhDyGP7fg.jpeg\" /><figcaption>Illustration Â© by LevÂ Polyakov</figcaption></figure><p>In collaboration with <a href=\"https://medium.com/@joe_19459\">JoeÂ Elliott</a>.</p><p>In distributed tracing, sampling is frequently used to reduce the number of traces that are collected and stored in the backend. This is often desirable because it is easy to produce more data than can be efficiently stored and queried. Sampling allows us to store only a subset of the total traces produced.</p><p>Traditionally Jaeger SDKs have supported a variety of <a href=\"https://www.jaegertracing.io/docs/latest/sampling/\">sampling techniques</a>. Our favorite has always been so called <em>remote sampling</em>, a feature pioneered in open source by the Jaeger project. In this setup, the Jaeger SDKs would query the Jaeger backend to retrieve a configuration of sampling rules for the given service, up to the granularity of individual endpoints. This can be a very powerful method of sampling as it can give operators central control of sampling rates across an entire organization.</p><p>Until recently, the only way to control which sampling rules are returned by the backend in the remote sampling mode was with a configuration file provided to the collector via the --sampling.strategies-file flag. Usually, the operators must manually update this file to push out different sampling rules. <em>Adaptive sampling</em>, added in v1.27.0, allows the collectors to automatically adjust sampling rates to meet preconfigured goals, by observing the current traffic in the system and the number of traces collected. This feature has been in production at Uber for several years, and is finally available on the open source version of Jaeger. A special shout-out to <a href=\"https://medium.com/@joe_19459\">Joe Elliott</a> for completing the adoption of the code contributed by Uber, which was sitting in the Jaeger repo for two years without being wired into the collectorâ€™s main.</p><h3>Why do we need remote &amp; adaptive sampling?</h3><p>It is always possible to configure the SDK to apply a very simple sampling strategy like a coin-flip decision, also known as probabilistic sampling. This might work fine in a small application, but when your architecture is measured in 100s or even 1000s of services, which all have different volumes of traffic, a single sampling probability for every service does not work that well, and configuring it individually for each service is a deployment nightmare. Remote sampling addresses this problem by centralizing all sampling configuration in the Jaeger collectors, where changes can be pushed out quickly to anyÂ service.</p><p>However, configuring sampling rules for every service manually, even if centrally, is still very tedious. Adaptive sampling takes this a step further and transforms this into a <em>declarative</em> configuration, where the operator only needs to set the target rate of trace collection, and the adaptive sampling engine dynamically adjusts the sampling rates individually for each service and each endpoint.</p><p>Another benefit of adaptive sampling is that it can automatically react to changes in the traffic. Many online services exhibit fluctuations in traffic during the day, e.g. Uber would have higher volume of requests during peak hours. Adaptive sampling engine would automatically adjust the sampling rates to keep the volume of trace data stable and within our samplingÂ budget.</p><h3>How to set up adaptive sampling?</h3><p>First, adaptive sampling requires that the Jaeger SDKs reach out to the backend to request the remote sampling document. This can be configured using environment variables. Please refer to the <a href=\"https://www.jaegertracing.io/docs/1.30/client-features/\">client features documentation</a> to confirm that these are supported by your JaegerÂ client.</p><pre>JAEGER_SAMPLER_TYPE=remote<br>JAEGER_SAMPLING_ENDPOINT=&lt;sampling endpoint on the jaeger agent&gt;</pre><p>The defaults are generally set up to work with a local Jaeger agent, running as a host agent or a sidecar, so itâ€™s possible your setup is already close to working. Jaeger SDK configuration actually defaults toÂ this:</p><pre>JAEGER_SAMPLER_TYPE=remote<br>JAEGER_SAMPLING_ENDPOINT=http://127.0.0.1:5778/sampling</pre><p>After your clients are configured you will need to make sure that your collectors are configured correctly to store adaptive sampling information. Currently, Jaeger uses the same storage for adaptive sampling as span storage and the only supported storage options for adaptive sampling are cassandra (since v.1.27) and memory (since v1.28). Using the environment variables to configure your collector may look something like:</p><pre>SPAN_STORAGE_TYPE=cassandra<br>SAMPLING_CONFIG_TYPE=adaptive</pre><p>If youâ€™re just getting started, we encourage you to check out this simple <a href=\"https://github.com/jaegertracing/jaeger/blob/50764f9ee4ac2d897331565066064ad2a3865fc9/docker-compose/jaeger-docker-compose.yml\">docker-compose example</a> which starts up Jaeger in a configuration that supports adaptive sampling.</p><p>The adaptive sampling algorithm can be tuned with a number of parameters that can be found in the <a href=\"https://www.jaegertracing.io/docs/1.30/cli/#jaeger-collector\">documentation</a>, or form the helpÂ command:</p><pre>$ docker run --rm \\<br>  -e SAMPLING_CONFIG_TYPE=adaptive \\<br>  jaegertracing/jaeger-collector:1.30 \\<br>  help | grep -e &#39;--sampling.&#39;</pre><pre>      --sampling.aggregation-buckets int                          Amount of historical data to keep in memory. (default 10)<br>      --sampling.buckets-for-calculation int                      This determines how much of the previous data is used in calculating the weighted QPS, ie. if BucketsForCalculation is 1, only the most recent data will be used in calculating the weighted QPS. (default 1)<br>      --sampling.calculation-interval duration                    How often new sampling probabilities are calculated. Recommended to be greater than the polling interval of your clients. (default 1m0s)<br>      --sampling.delay duration                                   Determines how far back the most recent state is. Use this if you want to add some buffer time for the aggregation to finish. (default 2m0s)<br>      --sampling.delta-tolerance float                            The acceptable amount of deviation between the observed samples-per-second and the desired (target) samples-per-second, expressed as a ratio. (default 0.3)<br>      --sampling.follower-lease-refresh-interval duration         The duration to sleep if this processor is a follower. (default 1m0s)<br>      --sampling.initial-sampling-probability float               The initial sampling probability for all new operations. (default 0.001)<br>      --sampling.leader-lease-refresh-interval duration           The duration to sleep if this processor is elected leader before attempting to renew the lease on the leader lock. This should be less than follower-lease-refresh-interval to reduce lock thrashing. (default 5s)<br>      --sampling.min-samples-per-second float                     The minimum number of traces that are sampled per second. (default 0.016666666666666666)<br>      --sampling.min-sampling-probability float                   The minimum sampling probability for all operations. (default 1e-05)<br>      --sampling.target-samples-per-second float                  The the global target rate of samples per operation. (default 1)</pre><h3>How does adaptive samplingÂ work?</h3><p>We start with some default sampling probability p assigned to every endpoint and a target rate R of traces we want to collect, such as 1 trace per second per endpoint. The collectors monitor the spans passing through them, looking for root spans of the traces started with this sampling policy, and calculate the actual rate of traces Râ€™ being collected. If Râ€™ &gt; R then our current probability for this endpoint is too high and needs to be reduced. Conversely, if Râ€™ &lt; R then we need to increase the probability. Since the actual traffic is always a bit noisy, the situation where Râ€™ == R rarely occurs, so the collector uses a certain tolerance threshold k such that the above rules are actually Râ€™ &gt; R + k and Râ€™ &lt; Râ€Šâ€”â€Šk. Once the new probability pâ€™ is calculated, the collector waits for a certain time interval to make sure it was retrieved by the SDKs and applied to new traces, then observes a new value of rate Râ€™ and repeats the cycle. Yuri Shkuroâ€™s book <a href=\"https://www.shkuro.com/books/2019-mastering-distributed-tracing/\">Mastering Distributed Tracing</a> contains a more detailed description of the math involved in the adaptive probability calculations implemented in the Jaeger collectors.</p><p>We also need to talk about how this is all done given that Jaeger allows us to run multiple collectors simultaneously. The adaptive sampling module implements a simple leader election mechanism using compare-and-swap operations supported by the storage backends. Each collector receives a distinct stream of spans from the services (remember, weâ€™re only interested in the root spans since that is where the sampling decision always happens), and maintains an in-memory aggregate of trace counts for each service / endpoint pair. Then after a certain time interval each collector writes this data (referred to as throughput in the code) to the storage backend. Then the collector that won the leader election reads all throughput data from storage for a given time range, aggregates it, performs the probability calculations, and writes the new probabilities summary for all services back to storage. The other collectors load that summary and use it to serve the requests for sampling strategies from the SDKs. Note that the leader election in this model is purely an optimization, because the sampling summary is written under a stable time-based key known to all collectors, so if more than one collector happens to perform the calculation of the probabilities, they would just override each otherâ€™s writes with the sameÂ data.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*qtt02KOzwajdyn1cFbMcfw.png\" /><figcaption>High level architecture of Jaegerâ€™s adaptive samplingÂ engine.</figcaption></figure><h3>Whatâ€™s missing?</h3><p>There are a few features on our wishlist that would make adaptive sampling better. One is the ability to account for the total number of spans instead of the total number of traces. Different endpoints can result in very different sizes of traces, even by several orders of magnitude. Yet the current implementation is only built around counting traces. It can be extended with additional heuristics, such as calculating the average trace size per endpoint offline and providing the adaptive sampling engine with a weights matrix to take into account when computing actual throughput.</p><p>Another nice-to-have feature, which actually requires changes in the remote sampling configuration, is to use other dimensions from trace data besides service name and endpoint name that are currently hardcoded in theÂ schema.</p><p>And yet another useful extension would be a configuration mechanism to allow overriding the target throughput rateR for specific services / endpoints, instead of using a single global parameter, because some services may be more important to your business and you might want to collect more data for them, or perhaps this could be a temporary setting due to some investigation.</p><h3>Wrapping up</h3><p>We are pleased to release the first open source end-to-end implementation of adaptive sampling for the Jaeger community. Please give it a shot, provide feedback and let us continue iterating on thisÂ feature.</p><p>If youâ€™re interested in contributing to the future development of this feature, there are a couple of areas where we could use some help immediately:</p><ol><li>Supporting ElasticSearch / OpenSearch as the backend for storing adaptive samplingÂ data.</li><li>Decoupling Jaeger storage configuration so that different storage backends could be used for span storage and adaptive sampling.</li></ol><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=50f336f4334\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/jaegertracing/adaptive-sampling-in-jaeger-50f336f4334\">Adaptive Sampling in Jaeger</a> was originally published in <a href=\"https://medium.com/jaegertracing\">JaegerTracing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        },
        {
          "title": "Migrating from Jaeger client to OpenTelemetry SDK",
          "link": "https://medium.com/jaegertracing/migrating-from-jaeger-client-to-opentelemetry-sdk-bd337d796759?source=rss----99735986d50---4",
          "guid": {
            "+content": "https://medium.com/p/bd337d796759",
            "+@isPermaLink": "false"
          },
          "category": [
            "opentelemetry",
            "opentracing",
            "distributed-tracing",
            "jaeger"
          ],
          "creator": "Juraci PaixÃ£o KrÃ¶hling",
          "pubDate": "Tue, 26 Oct 2021 08:07:06 GMT",
          "updated": "2022-01-17T16:24:37.513Z",
          "encoded": "<h3>Migrating from Jaeger Java client to OpenTelemetry SDK</h3><p>A couple of years ago, the <a href=\"https://opentelemetry.io/\">OpenTelemetry</a> project was founded by the merger of two similarly aimed projects: <a href=\"https://opentracing.io/\">OpenTracing</a> and <a href=\"https://opencensus.io/\">OpenCensus</a>. One of the goals of this new project was to create an <a href=\"https://medium.com/opentracing/merging-opentracing-and-opencensus-f0fe9c7ca6f0\">initial version that would â€œjust workâ€ with existing applications</a> instrumented using OpenTracing and OpenCensus.</p><p>On the Jaeger community, we decided some time ago that we would start recommending users to migrate to OpenTelemetry SDK once there was feature-parity with our existing clients, and we believe this time hasÂ come.</p><p>In the next few months, weâ€™ll be deprecating our own clients in favor of the OpenTelemetry SDK. With this, we believe weâ€™ll remain focused on the backend side of our tracing solution, leaving a bigger community to provide and support clients in a myriad of languages.</p><p>This guide will help you with your first steps in migrating to OpenTelemetry SDK from Jaeger for your application instrumented using the OpenTracing API. For the longer term, we recommend that you get familiar with the OpenTelemetry API and start using it to instrument your applications. We also recommend that you get familiar with the OpenTelemetry SDK and understand its features and limitations.</p><h3>Our instrumented application</h3><p>For this guide, weâ€™ll be using <a href=\"https://github.com/yurishkuro/opentracing-tutorial\">Yuri Shkuroâ€™s OpenTracing tutorial</a> as the starting point. More specifically, <a href=\"https://github.com/yurishkuro/opentracing-tutorial/tree/master/java/src/main/java/lesson04/solution\">the solution for lesson 4</a>. Before we start the migration, letâ€™s do a sanity check. Fork and clone that repository, and run each one of the following commands on its ownÂ console:</p><pre>$ ./run.sh lesson04.solution.Formatter server<br>$ ./run.sh lesson04.solution.Publisher server<br>$ podman run --rm --name jaeger -p 6831:6831/udp -p 14250:14250 -p 16686:16686 -p 14268:14268 jaegertracing/all-in-one:1.27</pre><p>If you donâ€™t have podman, replace it with docker in the last command. Even though we are using a recent version of the Jaeger backend, any version bigger than v1.8.2 (2018-11-28) shouldÂ work.</p><p>Once all the servers are running, execute theÂ client:</p><pre>$ ./run.sh lesson04.solution.Hello Bryan Bonjour</pre><p>You should now see a trace in <a href=\"http://localhost:16686\">your local Jaeger instance</a> similar to the image below. If this is the case, youâ€™re good to go. Stop the Formatter and the Publisher servers, but leave JaegerÂ running.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*kHD8sLyWsbEcR0lqMhFFNA.png\" /><figcaption>A trace generated by the Jaeger Client forÂ Java</figcaption></figure><h3>Adding the OpenTelemetry shim</h3><p>Before we can start using the shim, we need to add two BOMs (Bill of Materials) to our application. The first one contains the stable components for the OpenTelemetry Java SDK like the actual SDK, the API, and a few exporters, where the second has components that might not have the same stability or criticality, like the shim or the semantic conventions library.</p><pre>&lt;dependencyManagement&gt;<br>    &lt;dependencies&gt;<br>        &lt;dependency&gt;<br>            &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;<br>            &lt;artifactId&gt;opentelemetry-bom&lt;/artifactId&gt;<br>            &lt;version&gt;1.7.0&lt;/version&gt;<br>            &lt;type&gt;pom&lt;/type&gt;<br>            &lt;scope&gt;import&lt;/scope&gt;<br>        &lt;/dependency&gt;<br>        &lt;dependency&gt;<br>            &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;<br>            &lt;artifactId&gt;opentelemetry-bom-alpha&lt;/artifactId&gt;<br>            &lt;version&gt;1.7.0-alpha&lt;/version&gt;<br>            &lt;type&gt;pom&lt;/type&gt;<br>            &lt;scope&gt;import&lt;/scope&gt;<br>        &lt;/dependency&gt;<br>    &lt;/dependencies&gt;<br>&lt;/dependencyManagement&gt;</pre><p>Next, we add the dependencies related to the OpenTelemetry SDK to ourÂ project:</p><pre>&lt;dependency&gt;<br>    &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;<br>    &lt;artifactId&gt;opentelemetry-opentracing-shim&lt;/artifactId&gt;<br>&lt;/dependency&gt;<br><br>&lt;dependency&gt;<br>    &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;<br>    &lt;artifactId&gt;opentelemetry-semconv&lt;/artifactId&gt;<br>&lt;/dependency&gt;<br><br>&lt;dependency&gt;<br>    &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;<br>    &lt;artifactId&gt;opentelemetry-exporter-jaeger&lt;/artifactId&gt;<br>&lt;/dependency&gt;<br><br>&lt;dependency&gt;<br>    &lt;groupId&gt;io.opentelemetry&lt;/groupId&gt;<br>    &lt;artifactId&gt;opentelemetry-extension-trace-propagators&lt;/artifactId&gt;<br>&lt;/dependency&gt;</pre><ul><li>io.opentelemetry:opentelemetry-opentracing-shim is our shim, an OpenTracing Tracer implementation that will serve as the drop-in replacement for our JaegerÂ client.</li><li>io.opentelemetry:opentelemetry-semconv is a convenience library for adding attributes based on the semantic conventions.</li><li>io.opentelemetry:opentelemetry-exporter-jaeger is an exporter that sends data to our Jaeger instance. Note that there are <a href=\"https://github.com/open-telemetry/opentelemetry-java/issues/1387\">no exporters sending Thrift data over UDP</a>, which was the default encoding and transport for most Jaeger clients. Even though there is a Thrift HTTP exporter, we recommend using the gRPC exporter. Take a moment also to review the <a href=\"https://opentelemetry.io/registry/?language=java&amp;component=exporter\">available exporters</a>.</li><li>io.opentelemetry:opentelemetry-extension-trace-propagators contains the Jaeger propagator.</li></ul><p>We also need a couple of gRPC dependencies. We are using netty-shaded as the underlying transport, but make sure to learn about the alternatives and pick the appropriate one for your scenario.</p><pre>&lt;dependency&gt;<br>    &lt;groupId&gt;io.grpc&lt;/groupId&gt;<br>    &lt;artifactId&gt;grpc-protobuf&lt;/artifactId&gt;<br>    &lt;version&gt;1.41.0&lt;/version&gt;<br>&lt;/dependency&gt;<br>&lt;dependency&gt;<br>    &lt;groupId&gt;io.grpc&lt;/groupId&gt;<br>    &lt;artifactId&gt;grpc-netty-shaded&lt;/artifactId&gt;<br>    &lt;version&gt;1.41.0&lt;/version&gt;<br>&lt;/dependency&gt;</pre><p>At this point, we can remove the io.jaegertracing:jaeger-client dependency from ourÂ project.</p><h3>Replacing theÂ tracer</h3><p>With the dependencies in place, we can now replace the Tracer in our application with the OpenTelemetry Tracer. Most applications should have used the Tracer interface from the OpenTracing API in the method signatures instead of referencing the JaegerTracer directly. This makes our job easier now, as our changes are localized to the lib.Tracing class, initÂ method.</p><p>The first step in our refactoring will be to remove the entire implementation and just return null. This is located in the java/src/main/java/lib/Tracing.java file.</p><pre>public static Tracer init(String service) {<br>    return null;<br>}</pre><p>Take the opportunity to also remove the imports starting with io.jaegertracing.</p><p>Weâ€™ll now create the Resource attribute holding the service name for our application. In the init method, add the following:</p><pre>Resource serviceNameResource = Resource.create(Attributes.of(ResourceAttributes.SERVICE_NAME, service));</pre><p>We now initialize a gRPC channel with our Jaeger collector. In the init method, add the following:</p><pre>ManagedChannel jaegerChannel = ManagedChannelBuilder<br>    .forAddress(&quot;localhost&quot;, 14250)<br>    .usePlaintext()<br>    .build();</pre><p>We can now create the Jaeger exporter:</p><pre>JaegerGrpcSpanExporter jaegerExporter = JaegerGrpcSpanExporter.builder()<br>    .setChannel(jaegerChannel)<br>    .setTimeout(1, TimeUnit.SECONDS)<br>    .build();</pre><p>Creating the Tracer provider is the nextÂ step:</p><pre>SdkTracerProvider tracerProvider = SdkTracerProvider.builder()<br>    .addSpanProcessor(SimpleSpanProcessor.create(jaegerExporter))<br>    .setResource(Resource.getDefault().merge(serviceNameResource))<br>    .build();</pre><p>We create an OpenTelemetry SDK instance with our tracer provider and the context propagators. To keep backward compatibility with existing services, we added the JaegerPropagator. For the long run though, we might want to start using the W3CTraceContextPropagator instead. Given that not all services are going to be updated at once, it&#39;s a good idea to run with both propagators during the transition time. The side-effect is that we&#39;ll end up having a bigger HTTP request between our microservices, but hopefully that&#39;s not big enough to have a considerable impact.</p><pre>OpenTelemetrySdk openTelemetry = OpenTelemetrySdk.builder()<br>    .setPropagators(ContextPropagators.create(<br>        TextMapPropagator.composite(<br>            W3CTraceContextPropagator.getInstance(),<br>            JaegerPropagator.getInstance()<br>        )<br>    ))<br>    .setTracerProvider(tracerProvider)<br>    .build();</pre><p>As a good practice, we try to close our tracer provider when the JVM is shuttingÂ down:</p><pre>Runtime.getRuntime().addShutdownHook(new Thread(tracerProvider::close));</pre><p>And finally, we wrap our OpenTelemetry SDK instance in our shim, returning it toÂ callers:</p><pre>return OpenTracingShim.createTracerShim(openTelemetry);</pre><h3>Trying itÂ out</h3><p>Our migration should be ready by now, so, letâ€™s try it out by running the same commands asÂ before:</p><pre>./run.sh lesson04.solution.Formatter server<br>./run.sh lesson04.solution.Publisher server<br>./run.sh lesson04.solution.Hello Bryan Bonjour</pre><p>At this point, we should have a new trace in our Jaeger instance, created by the OpenTelemetry SDK: confirm this is the case by checking the otel.library.name tag and telemetry.sdk.name process attribute, like in the following image:</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*P-ntq0qqfcy0qGcV2rkD3g.png\" /><figcaption>A trace generated by the OpenTelemetry SDK forÂ Java</figcaption></figure><h3>Remote sampling</h3><p>If you are using the remotely controlled sampling configuration in the Jaeger client, you should double-check if the language you are using supports it already. Even though the <a href=\"https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/sdk-environment-variables.md#general-sdk-configuration\">jaeger_remote sampler is a valid value for the env var </a><a href=\"https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/sdk-environment-variables.md#general-sdk-configuration\">OTEL_TRACES_SAMPLER</a> as per the OpenTelemetry SDK specification, it is not supported by most languages yet. At the moment of this writing, only the OpenTelemetry Java SDK supportsÂ it.</p><h3>Wrapping up</h3><p>This migration was simple and without a lot of work: a localized refactor of the init method was all it took. A bigger migration is certainly switching from the OpenTracing API to the OpenTelemetry API, but with the shim in place, you can take an opportunistic approach and migrate the APIs only for the services that are undergoing some maintenance or refactoring already.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=bd337d796759\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/jaegertracing/migrating-from-jaeger-client-to-opentelemetry-sdk-bd337d796759\">Migrating from Jaeger client to OpenTelemetry SDK</a> was originally published in <a href=\"https://medium.com/jaegertracing\">JaegerTracing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        },
        {
          "title": "Jaeger Persistent Storage With Elasticsearch, Cassandra & Kafka",
          "link": "https://medium.com/jaegertracing/jaeger-persistent-storage-with-elasticsearch-cassandra-kafka-f3c6c680a58c?source=rss----99735986d50---4",
          "guid": {
            "+content": "https://medium.com/p/f3c6c680a58c",
            "+@isPermaLink": "false"
          },
          "category": [
            "jaegertracing",
            "storage",
            "distributed-tracing",
            "high-availability",
            "devops"
          ],
          "creator": "Dotan Horovits (@horovits)",
          "pubDate": "Wed, 24 Feb 2021 16:29:22 GMT",
          "updated": "2021-02-28T16:26:15.774Z",
          "encoded": "<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*6wjV2PGdDbt1z_sNHNVokQ.jpeg\" /></figure><p>Running systems in production involves requirements for high availability, resilience and recovery from failure. When running cloud native applications this becomes even more critical, as the base assumption in such environments is that compute nodes will suffer outages, Kubernetes nodes will go down and microservices instances are likely to fail, yet the service is expected to remain up andÂ running.</p><p>In a recent post, I presented the different Jaeger components and best practices for <a href=\"https://medium.com/jaegertracing/a-guide-to-deploying-jaeger-on-kubernetes-in-production-69afb9a7c8e5\">deploying Jaeger in production</a>. In that post, I mentioned that Jaeger uses external services for ingesting and persisting the span data, such as Elasticsearch, Cassandra and Kafka. This is due to the fact that the Jaeger Collector is a stateless service and you need to point it to some sort of storage to which it will forward the spanÂ data.</p><p>In this post, Iâ€™d like to discuss how to ingest and persist Jaeger trace data in production to ensure resilience and high availability, and the external services you need to set up for that. Iâ€™llÂ cover:</p><ul><li>Standard persistent storage for Jaeger with Elasticsearch and Cassandra</li><li>Alternative persistent storage with gRPCÂ plugin</li><li>Handling high load tracing data streams withÂ Kafka</li><li>Jaeger persistence during development with<a href=\"https://www.jaegertracing.io/docs/1.18/cli/#jaeger-all-in-one\"> jaegertracing all-in-one</a></li></ul><h3>Deploying Jaeger with Elasticsearch, Kafka or other ExternalÂ Services</h3><p>Jaeger deployments may involve additional services such as Elasticsearch, Cassandra and Kafka. But do these services come as part of Jaegerâ€™s installation and how are these services deployed?</p><p>The Jaeger Operator and Jaegerâ€™s Helm chart (see Jaegerâ€™s deployment tools in <a href=\"https://logz.io/blog/jaeger-kubernetes-best-practices/?utm_source=jaegerblog&amp;utm_medium=social&amp;utm_campaign=community_outreach&amp;utm_content=jaeger\">this post</a>) offer the option of a self-provisioned Elasticsearch/Cassandra/Kafka cluster (in which Jaeger deployment also deploys these clusters), as well as the option of connecting to an existingÂ cluster.</p><p>The self-provisioned option offers a good starting point, but you may prefer to deploy these services independently for better flexibility and control over the way these clusters are deployed, managed, monitored, upgraded and secured, in accordance with your teamâ€™s DevOps practices. In particular, if you are already running a Kafka or Elasticsearch cluster, it may make more sense to re-use these infrastructure components rather than maintain a separateÂ cluster.</p><h3>Elasticsearch vs. Cassandra as Jaeger BackendÂ Storage</h3><p>For production deployments, Jaeger currently provides built-in support for two storage solutions, both of which are very popular open source NoSQL databases: Elasticsearch and Cassandra. The Jaeger collector and query service need to be configured with the storage solution of choice so they can write to it and query it. You can pass the desired storage type and the database endpoint via environment variables. For example, a basic Elasticsearch setup will define the following environment variables:</p><pre>SPAN_STORAGE_TYPE=elasticsearch<br>ES_SERVER_URLS=&lt;...&gt;</pre><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/1*mC9p0H_o-JcEWWBiGubj8w.png\" /><figcaption>Illustration of direct-to-storage architecture. Source: jaegertracing.io</figcaption></figure><p>So which storage backend should you use: Elasticsearch or Cassandra?</p><p>The Jaeger team provides a clear <strong>recommendation to use Elasticsearch as the storage backend</strong> over Cassandra. And they have very goodÂ <a href=\"https://www.jaegertracing.io/docs/1.18/faq/\">reasons</a>:</p><ul><li>Cassandra is a key-value database, so it is more efficient for retrieving traces by trace ID, but it does not provide the same powerful search capabilities as Elasticsearch. Effectively, the Jaeger backend implements the search functionality on the client side, on top of k-v storage, which is limited and may produce inconsistent results (see <a href=\"https://github.com/jaegertracing/jaeger/issues/166\">issue-166</a> for more details). Elasticsearch does not suffer from these issues, resulting in better usability. Elasticsearch can also be queried directly, e.g. from Kibana dashboards, and provide useful analytics and aggregations.</li><li>Based on past performance experiments we observed single writes to be much faster in Cassandra than Elasticsearch, which might suggest that it may sustain higher write throughput. However, because the Jaeger backend needs to implement search capability on top of k-v storage, writing spans to Cassandra is actually subject to large write amplification: in addition to writing a record for the span itself, Jaeger performs extra writes for service name and operation name indexing, as well as extra index writes for every tag. In contrast, saving a span to Elasticsearch is a single write, and all indexing takes place inside the ES node. As a result, the overall throughput to Cassandra is comparable with Elasticsearch.</li></ul><p>One benefit of Cassandra backend is simplified maintenance due to its native support for data TTL. In Elasticsearch the data expiration is managed through index rotation, which requires additional setup (see <a href=\"https://logz.io/blog/managing-elasticsearch-indices/#elasticsearch-rollover\">Elasticsearch Rollover</a>).</p><h3>Alternative Persistent Storage forÂ Jaeger</h3><p>In addition to Jaegerâ€™s built-in support for Elasticsearch and Cassandra, Jaeger supports a <a href=\"https://github.com/jaegertracing/jaeger/tree/master/plugin/storage/grpc\">gRPC plugin</a> (SPAN_STORAGE_TYPE=grpc-plugin) which enables developing custom plugins to other storage types. The Jaeger community currently offers integrations with several persistent storage types, four of which are defined as â€˜availableâ€™ at present: ScyllaDB, InfluxDB, Couchbase and <a href=\"https://github.com/logzio/jaeger-logzio\">Logz.io</a> (disclaimer: I work at Logz.io).</p><p>Other integrations, which are not yet available, include NoSQL data stores from the big cloud vendors such as Amazon DynamoDB, Azure CosmosDB and Google BigTable, as well as popular SQL databases MySQL and PostgreSQL. You can check out the list of additional storage backends and updated status on this Jaeger GitHubÂ <a href=\"https://github.com/jaegertracing/jaeger/issues/638\">issue</a>.</p><h3>Using Kafka to Ingest High-Load Jaeger SpanÂ Data</h3><p>If you monitor many microservices, if you have a high volume of span data, or if your system generates data bursts on occasions, then your external backend storage may not be able to handle the load and may become a bottleneck, impacting the overall performance. In such cases you should employ the streaming deployment strategy that I mentioned in the previous post which uses Kafka between the Collector and the storage to buffer the span data from the Jaeger Collector.</p><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/960/1*QqyTQNP1GI4jQhNyGWF1pA.png\" /><figcaption>Illustration of architecture with Kafka as intermediate buffer. Source: jaegertracing.io</figcaption></figure><p>In this case, you configure Kafka as the target for Jaeger Collector ( SPAN_STORAGE_TYPE=kafka ) as well as the relevant Kafka brokers, topic and other parameters.</p><p>Iâ€™d like to stress that Kafka is not an alternative backend storage (although the setting SPAN_STORAGE_TYPE=kafka may be confusing). Your Jaeger backend still needs a backend storage as described in the previous sections, with Kafka serving as a buffer to take off pressure.</p><p>To support the streaming deployment Jaeger project also offers the Jaeger Ingester service, which can asynchronously read from Kafka topic and write to the storage backend (Elasticsearch or Cassandra). Of course, you can choose to implement your own service to do the same, if you need a particular target storage or ingestion strategy.</p><h3>Jaeger Persistence During Development with jaegertracing All-in-One</h3><p>Up till now I discussed production deployment. However, if you are exploring Jaeger or are doing a small PoC or development, then you are probably using Jaegerâ€™s All-in-One installation, and you may be wondering how this is applicable toÂ you.</p><p>All-in-one is a single node installation, in which you donâ€™t trouble yourself with non-functional requirements such as resilience or scalability. In an all-in-one deployment, Jaeger uses in-memory persistence by default. Alternatively, you can choose to use Badger, which provides a single-node storage based on a filesystem (similar to Prometheus model). You can find more details on using BadgerÂ <a href=\"https://www.jaegertracing.io/docs/latest/deployment/#badger---local-storage\">here</a>.</p><p>Bear in mind that both in-memory and Badger are meant for all-in-one deployments only, and are not suitable for production deployments.</p><h3>Endnote</h3><p>When deploying <a href=\"https://logz.io/blog/jaeger-kubernetes-best-practices/?utm_source=jaegerblog&amp;utm_medium=social&amp;utm_campaign=community_outreach&amp;utm_content=jaeger\">Jaeger in production</a>, you need to address data persistence, high availability and scalability concerns. In order to address these concerns you need to deploy additional services.</p><p>First of all, you should deploy and configure an external persistence storage for your span data. The recommended persistence storage for Jaeger in production is Elasticsearch.</p><p>Secondly, when dealing with high load of span data, you should deploy Kafka in front of the storage to handle the ingestion and provide backpressure.</p><p>Running in production entails many other considerations not covered in this post, such as upgrades to Jaeger components as well as Elasticsearch, Kafka or any additional service in the deployment; monitoring the different services, and securing access to these services.</p><img src=\"https://medium.com/_/stat?event=post.clientViewed&referrerSource=full_rss&postId=f3c6c680a58c\" width=\"1\" height=\"1\" alt=\"\"><hr><p><a href=\"https://medium.com/jaegertracing/jaeger-persistent-storage-with-elasticsearch-cassandra-kafka-f3c6c680a58c\">Jaeger Persistent Storage With Elasticsearch, Cassandra &amp; Kafka</a> was originally published in <a href=\"https://medium.com/jaegertracing\">JaegerTracing</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>"
        }
      ]
    }
  }
}
